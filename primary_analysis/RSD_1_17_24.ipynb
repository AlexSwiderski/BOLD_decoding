{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1519816a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from itertools import combinations\n",
    "from scipy.stats import pearsonr\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a0664ad6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the original neural and word2vec similarity matrices\n",
    "word2vec_sim_df = pd.read_csv('matrix_residuals_semdist15.csv')\n",
    "neural_sim_df = pd.read_csv('p1_sim.csv')\n",
    "\n",
    "#Align the columns of the neural similarity matrix to match the word2vec similarity matrix\n",
    "neural_sim_df_aligned = neural_sim_df[word2vec_sim_df.columns]\n",
    "\n",
    "# Reorder the rows of the neural similarity matrix to match the order in the word2vec similarity matrix\n",
    "neural_sim_df_reordered = neural_sim_df_aligned.set_index('term').reindex(word2vec_sim_df['term']).reset_index()\n",
    "# List of words (excluding the 'term' column)\n",
    "words = word2vec_sim_df.columns[1:]\n",
    "\n",
    "# Initialize a list to store results\n",
    "decoding_results = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "69c1949e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>term</th>\n",
       "      <th>horse</th>\n",
       "      <th>bear</th>\n",
       "      <th>cat</th>\n",
       "      <th>dog</th>\n",
       "      <th>cow</th>\n",
       "      <th>leg</th>\n",
       "      <th>arm</th>\n",
       "      <th>hand</th>\n",
       "      <th>foot</th>\n",
       "      <th>...</th>\n",
       "      <th>tomato</th>\n",
       "      <th>celery</th>\n",
       "      <th>corn</th>\n",
       "      <th>carrot</th>\n",
       "      <th>lettuce</th>\n",
       "      <th>bicycle</th>\n",
       "      <th>car</th>\n",
       "      <th>train</th>\n",
       "      <th>truck</th>\n",
       "      <th>airplane</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>horse</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.867668</td>\n",
       "      <td>0.794402</td>\n",
       "      <td>0.817715</td>\n",
       "      <td>0.762252</td>\n",
       "      <td>0.655147</td>\n",
       "      <td>0.560490</td>\n",
       "      <td>0.798085</td>\n",
       "      <td>0.773818</td>\n",
       "      <td>...</td>\n",
       "      <td>0.679341</td>\n",
       "      <td>0.517919</td>\n",
       "      <td>0.809874</td>\n",
       "      <td>0.586457</td>\n",
       "      <td>0.814620</td>\n",
       "      <td>0.886597</td>\n",
       "      <td>0.591752</td>\n",
       "      <td>0.698553</td>\n",
       "      <td>0.770612</td>\n",
       "      <td>0.830323</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>bear</td>\n",
       "      <td>0.867668</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.784209</td>\n",
       "      <td>0.856424</td>\n",
       "      <td>0.842841</td>\n",
       "      <td>0.576497</td>\n",
       "      <td>0.529558</td>\n",
       "      <td>0.703956</td>\n",
       "      <td>0.695968</td>\n",
       "      <td>...</td>\n",
       "      <td>0.678013</td>\n",
       "      <td>0.514908</td>\n",
       "      <td>0.710625</td>\n",
       "      <td>0.626575</td>\n",
       "      <td>0.799161</td>\n",
       "      <td>0.849823</td>\n",
       "      <td>0.696179</td>\n",
       "      <td>0.748991</td>\n",
       "      <td>0.761983</td>\n",
       "      <td>0.828290</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>cat</td>\n",
       "      <td>0.794402</td>\n",
       "      <td>0.784209</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.801168</td>\n",
       "      <td>0.706735</td>\n",
       "      <td>0.798449</td>\n",
       "      <td>0.518258</td>\n",
       "      <td>0.803878</td>\n",
       "      <td>0.672096</td>\n",
       "      <td>...</td>\n",
       "      <td>0.571532</td>\n",
       "      <td>0.317849</td>\n",
       "      <td>0.771336</td>\n",
       "      <td>0.417673</td>\n",
       "      <td>0.768428</td>\n",
       "      <td>0.835674</td>\n",
       "      <td>0.513325</td>\n",
       "      <td>0.669761</td>\n",
       "      <td>0.608832</td>\n",
       "      <td>0.677270</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>dog</td>\n",
       "      <td>0.817715</td>\n",
       "      <td>0.856424</td>\n",
       "      <td>0.801168</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.869596</td>\n",
       "      <td>0.669203</td>\n",
       "      <td>0.587157</td>\n",
       "      <td>0.763389</td>\n",
       "      <td>0.744822</td>\n",
       "      <td>...</td>\n",
       "      <td>0.644053</td>\n",
       "      <td>0.475084</td>\n",
       "      <td>0.698746</td>\n",
       "      <td>0.537472</td>\n",
       "      <td>0.784360</td>\n",
       "      <td>0.831094</td>\n",
       "      <td>0.610807</td>\n",
       "      <td>0.618325</td>\n",
       "      <td>0.649231</td>\n",
       "      <td>0.724007</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>cow</td>\n",
       "      <td>0.762252</td>\n",
       "      <td>0.842841</td>\n",
       "      <td>0.706735</td>\n",
       "      <td>0.869596</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.564517</td>\n",
       "      <td>0.602313</td>\n",
       "      <td>0.636055</td>\n",
       "      <td>0.691362</td>\n",
       "      <td>...</td>\n",
       "      <td>0.561507</td>\n",
       "      <td>0.544593</td>\n",
       "      <td>0.655788</td>\n",
       "      <td>0.623611</td>\n",
       "      <td>0.740062</td>\n",
       "      <td>0.792543</td>\n",
       "      <td>0.700272</td>\n",
       "      <td>0.650916</td>\n",
       "      <td>0.690289</td>\n",
       "      <td>0.769298</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>leg</td>\n",
       "      <td>0.655147</td>\n",
       "      <td>0.576497</td>\n",
       "      <td>0.798449</td>\n",
       "      <td>0.669203</td>\n",
       "      <td>0.564517</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.589304</td>\n",
       "      <td>0.745915</td>\n",
       "      <td>0.706871</td>\n",
       "      <td>...</td>\n",
       "      <td>0.324236</td>\n",
       "      <td>0.241527</td>\n",
       "      <td>0.740154</td>\n",
       "      <td>0.298281</td>\n",
       "      <td>0.635136</td>\n",
       "      <td>0.779629</td>\n",
       "      <td>0.405805</td>\n",
       "      <td>0.586700</td>\n",
       "      <td>0.483917</td>\n",
       "      <td>0.521596</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>arm</td>\n",
       "      <td>0.560490</td>\n",
       "      <td>0.529558</td>\n",
       "      <td>0.518258</td>\n",
       "      <td>0.587157</td>\n",
       "      <td>0.602313</td>\n",
       "      <td>0.589304</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.683080</td>\n",
       "      <td>0.846284</td>\n",
       "      <td>...</td>\n",
       "      <td>0.211426</td>\n",
       "      <td>0.550446</td>\n",
       "      <td>0.497993</td>\n",
       "      <td>0.652083</td>\n",
       "      <td>0.436008</td>\n",
       "      <td>0.698630</td>\n",
       "      <td>0.389628</td>\n",
       "      <td>0.465599</td>\n",
       "      <td>0.566975</td>\n",
       "      <td>0.611928</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>hand</td>\n",
       "      <td>0.798085</td>\n",
       "      <td>0.703956</td>\n",
       "      <td>0.803878</td>\n",
       "      <td>0.763389</td>\n",
       "      <td>0.636055</td>\n",
       "      <td>0.745915</td>\n",
       "      <td>0.683080</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.840091</td>\n",
       "      <td>...</td>\n",
       "      <td>0.528932</td>\n",
       "      <td>0.426071</td>\n",
       "      <td>0.744326</td>\n",
       "      <td>0.466841</td>\n",
       "      <td>0.739569</td>\n",
       "      <td>0.838479</td>\n",
       "      <td>0.416598</td>\n",
       "      <td>0.552607</td>\n",
       "      <td>0.609400</td>\n",
       "      <td>0.669398</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>foot</td>\n",
       "      <td>0.773818</td>\n",
       "      <td>0.695968</td>\n",
       "      <td>0.672096</td>\n",
       "      <td>0.744822</td>\n",
       "      <td>0.691362</td>\n",
       "      <td>0.706871</td>\n",
       "      <td>0.846284</td>\n",
       "      <td>0.840091</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.449388</td>\n",
       "      <td>0.578857</td>\n",
       "      <td>0.695290</td>\n",
       "      <td>0.649962</td>\n",
       "      <td>0.644539</td>\n",
       "      <td>0.824975</td>\n",
       "      <td>0.479248</td>\n",
       "      <td>0.553614</td>\n",
       "      <td>0.694302</td>\n",
       "      <td>0.709507</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>eye</td>\n",
       "      <td>0.811312</td>\n",
       "      <td>0.797959</td>\n",
       "      <td>0.739001</td>\n",
       "      <td>0.761645</td>\n",
       "      <td>0.764978</td>\n",
       "      <td>0.678523</td>\n",
       "      <td>0.614393</td>\n",
       "      <td>0.756584</td>\n",
       "      <td>0.762380</td>\n",
       "      <td>...</td>\n",
       "      <td>0.518579</td>\n",
       "      <td>0.535730</td>\n",
       "      <td>0.842577</td>\n",
       "      <td>0.609926</td>\n",
       "      <td>0.822865</td>\n",
       "      <td>0.888571</td>\n",
       "      <td>0.742480</td>\n",
       "      <td>0.823249</td>\n",
       "      <td>0.822879</td>\n",
       "      <td>0.826608</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>apartment</td>\n",
       "      <td>0.698721</td>\n",
       "      <td>0.762514</td>\n",
       "      <td>0.596658</td>\n",
       "      <td>0.621620</td>\n",
       "      <td>0.608525</td>\n",
       "      <td>0.445907</td>\n",
       "      <td>0.231247</td>\n",
       "      <td>0.482260</td>\n",
       "      <td>0.448296</td>\n",
       "      <td>...</td>\n",
       "      <td>0.587824</td>\n",
       "      <td>0.295193</td>\n",
       "      <td>0.649879</td>\n",
       "      <td>0.399071</td>\n",
       "      <td>0.730971</td>\n",
       "      <td>0.689024</td>\n",
       "      <td>0.722454</td>\n",
       "      <td>0.822750</td>\n",
       "      <td>0.710393</td>\n",
       "      <td>0.733315</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>igloo</td>\n",
       "      <td>0.661859</td>\n",
       "      <td>0.688277</td>\n",
       "      <td>0.670116</td>\n",
       "      <td>0.640349</td>\n",
       "      <td>0.679977</td>\n",
       "      <td>0.570472</td>\n",
       "      <td>0.403760</td>\n",
       "      <td>0.536716</td>\n",
       "      <td>0.530904</td>\n",
       "      <td>...</td>\n",
       "      <td>0.415552</td>\n",
       "      <td>0.403774</td>\n",
       "      <td>0.759893</td>\n",
       "      <td>0.475588</td>\n",
       "      <td>0.739594</td>\n",
       "      <td>0.736073</td>\n",
       "      <td>0.753110</td>\n",
       "      <td>0.824186</td>\n",
       "      <td>0.746618</td>\n",
       "      <td>0.724672</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>barn</td>\n",
       "      <td>0.711925</td>\n",
       "      <td>0.737495</td>\n",
       "      <td>0.622361</td>\n",
       "      <td>0.619353</td>\n",
       "      <td>0.651241</td>\n",
       "      <td>0.470127</td>\n",
       "      <td>0.356480</td>\n",
       "      <td>0.513918</td>\n",
       "      <td>0.526572</td>\n",
       "      <td>...</td>\n",
       "      <td>0.560346</td>\n",
       "      <td>0.375587</td>\n",
       "      <td>0.661230</td>\n",
       "      <td>0.501225</td>\n",
       "      <td>0.706021</td>\n",
       "      <td>0.714799</td>\n",
       "      <td>0.758685</td>\n",
       "      <td>0.815631</td>\n",
       "      <td>0.817320</td>\n",
       "      <td>0.801912</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>house</td>\n",
       "      <td>0.635954</td>\n",
       "      <td>0.703997</td>\n",
       "      <td>0.529147</td>\n",
       "      <td>0.588550</td>\n",
       "      <td>0.651021</td>\n",
       "      <td>0.356240</td>\n",
       "      <td>0.172805</td>\n",
       "      <td>0.388721</td>\n",
       "      <td>0.360437</td>\n",
       "      <td>...</td>\n",
       "      <td>0.512642</td>\n",
       "      <td>0.301658</td>\n",
       "      <td>0.615431</td>\n",
       "      <td>0.407944</td>\n",
       "      <td>0.699130</td>\n",
       "      <td>0.633268</td>\n",
       "      <td>0.788316</td>\n",
       "      <td>0.815476</td>\n",
       "      <td>0.734614</td>\n",
       "      <td>0.730749</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>church</td>\n",
       "      <td>0.632664</td>\n",
       "      <td>0.686622</td>\n",
       "      <td>0.568706</td>\n",
       "      <td>0.601965</td>\n",
       "      <td>0.621611</td>\n",
       "      <td>0.481075</td>\n",
       "      <td>0.309655</td>\n",
       "      <td>0.449913</td>\n",
       "      <td>0.464908</td>\n",
       "      <td>...</td>\n",
       "      <td>0.460718</td>\n",
       "      <td>0.367576</td>\n",
       "      <td>0.673649</td>\n",
       "      <td>0.421551</td>\n",
       "      <td>0.727649</td>\n",
       "      <td>0.695612</td>\n",
       "      <td>0.714085</td>\n",
       "      <td>0.826501</td>\n",
       "      <td>0.705087</td>\n",
       "      <td>0.706253</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>door</td>\n",
       "      <td>0.664959</td>\n",
       "      <td>0.697948</td>\n",
       "      <td>0.707445</td>\n",
       "      <td>0.654743</td>\n",
       "      <td>0.614215</td>\n",
       "      <td>0.660156</td>\n",
       "      <td>0.389011</td>\n",
       "      <td>0.605931</td>\n",
       "      <td>0.573216</td>\n",
       "      <td>...</td>\n",
       "      <td>0.484830</td>\n",
       "      <td>0.277677</td>\n",
       "      <td>0.765617</td>\n",
       "      <td>0.370305</td>\n",
       "      <td>0.743025</td>\n",
       "      <td>0.744535</td>\n",
       "      <td>0.694549</td>\n",
       "      <td>0.837039</td>\n",
       "      <td>0.686668</td>\n",
       "      <td>0.699127</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>closet</td>\n",
       "      <td>0.715603</td>\n",
       "      <td>0.687266</td>\n",
       "      <td>0.673670</td>\n",
       "      <td>0.612249</td>\n",
       "      <td>0.605650</td>\n",
       "      <td>0.643443</td>\n",
       "      <td>0.464889</td>\n",
       "      <td>0.615073</td>\n",
       "      <td>0.621563</td>\n",
       "      <td>...</td>\n",
       "      <td>0.392241</td>\n",
       "      <td>0.312676</td>\n",
       "      <td>0.751869</td>\n",
       "      <td>0.463871</td>\n",
       "      <td>0.706825</td>\n",
       "      <td>0.801271</td>\n",
       "      <td>0.740680</td>\n",
       "      <td>0.883244</td>\n",
       "      <td>0.787451</td>\n",
       "      <td>0.748007</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>chimney</td>\n",
       "      <td>0.758883</td>\n",
       "      <td>0.759291</td>\n",
       "      <td>0.688216</td>\n",
       "      <td>0.698518</td>\n",
       "      <td>0.717023</td>\n",
       "      <td>0.673674</td>\n",
       "      <td>0.513790</td>\n",
       "      <td>0.589217</td>\n",
       "      <td>0.669381</td>\n",
       "      <td>...</td>\n",
       "      <td>0.451119</td>\n",
       "      <td>0.457083</td>\n",
       "      <td>0.772736</td>\n",
       "      <td>0.514505</td>\n",
       "      <td>0.763782</td>\n",
       "      <td>0.831779</td>\n",
       "      <td>0.743953</td>\n",
       "      <td>0.859954</td>\n",
       "      <td>0.767059</td>\n",
       "      <td>0.789290</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>window</td>\n",
       "      <td>0.762504</td>\n",
       "      <td>0.718538</td>\n",
       "      <td>0.666203</td>\n",
       "      <td>0.622478</td>\n",
       "      <td>0.579022</td>\n",
       "      <td>0.610803</td>\n",
       "      <td>0.376803</td>\n",
       "      <td>0.623167</td>\n",
       "      <td>0.589509</td>\n",
       "      <td>...</td>\n",
       "      <td>0.586896</td>\n",
       "      <td>0.428371</td>\n",
       "      <td>0.784558</td>\n",
       "      <td>0.458079</td>\n",
       "      <td>0.786179</td>\n",
       "      <td>0.769819</td>\n",
       "      <td>0.670236</td>\n",
       "      <td>0.804625</td>\n",
       "      <td>0.735202</td>\n",
       "      <td>0.742122</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>arch</td>\n",
       "      <td>0.610093</td>\n",
       "      <td>0.671840</td>\n",
       "      <td>0.491565</td>\n",
       "      <td>0.505957</td>\n",
       "      <td>0.506032</td>\n",
       "      <td>0.395335</td>\n",
       "      <td>0.280516</td>\n",
       "      <td>0.423684</td>\n",
       "      <td>0.435040</td>\n",
       "      <td>...</td>\n",
       "      <td>0.525820</td>\n",
       "      <td>0.316577</td>\n",
       "      <td>0.620526</td>\n",
       "      <td>0.438808</td>\n",
       "      <td>0.636221</td>\n",
       "      <td>0.630249</td>\n",
       "      <td>0.692751</td>\n",
       "      <td>0.800162</td>\n",
       "      <td>0.693101</td>\n",
       "      <td>0.709135</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>shirt</td>\n",
       "      <td>0.878576</td>\n",
       "      <td>0.834256</td>\n",
       "      <td>0.813089</td>\n",
       "      <td>0.840423</td>\n",
       "      <td>0.777312</td>\n",
       "      <td>0.678780</td>\n",
       "      <td>0.494433</td>\n",
       "      <td>0.778000</td>\n",
       "      <td>0.727838</td>\n",
       "      <td>...</td>\n",
       "      <td>0.741920</td>\n",
       "      <td>0.529024</td>\n",
       "      <td>0.819480</td>\n",
       "      <td>0.538137</td>\n",
       "      <td>0.886150</td>\n",
       "      <td>0.848812</td>\n",
       "      <td>0.582945</td>\n",
       "      <td>0.659794</td>\n",
       "      <td>0.675980</td>\n",
       "      <td>0.766515</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>pants</td>\n",
       "      <td>0.777598</td>\n",
       "      <td>0.736485</td>\n",
       "      <td>0.794516</td>\n",
       "      <td>0.776590</td>\n",
       "      <td>0.731372</td>\n",
       "      <td>0.805345</td>\n",
       "      <td>0.596718</td>\n",
       "      <td>0.750308</td>\n",
       "      <td>0.754078</td>\n",
       "      <td>...</td>\n",
       "      <td>0.492070</td>\n",
       "      <td>0.477946</td>\n",
       "      <td>0.858527</td>\n",
       "      <td>0.495714</td>\n",
       "      <td>0.809120</td>\n",
       "      <td>0.848815</td>\n",
       "      <td>0.570421</td>\n",
       "      <td>0.724547</td>\n",
       "      <td>0.629747</td>\n",
       "      <td>0.704129</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>skirt</td>\n",
       "      <td>0.822725</td>\n",
       "      <td>0.790998</td>\n",
       "      <td>0.769078</td>\n",
       "      <td>0.799265</td>\n",
       "      <td>0.749963</td>\n",
       "      <td>0.688187</td>\n",
       "      <td>0.486777</td>\n",
       "      <td>0.759455</td>\n",
       "      <td>0.678832</td>\n",
       "      <td>...</td>\n",
       "      <td>0.587417</td>\n",
       "      <td>0.492523</td>\n",
       "      <td>0.792462</td>\n",
       "      <td>0.491346</td>\n",
       "      <td>0.854875</td>\n",
       "      <td>0.833014</td>\n",
       "      <td>0.564682</td>\n",
       "      <td>0.670851</td>\n",
       "      <td>0.626155</td>\n",
       "      <td>0.729970</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>coat</td>\n",
       "      <td>0.889299</td>\n",
       "      <td>0.816334</td>\n",
       "      <td>0.796006</td>\n",
       "      <td>0.839576</td>\n",
       "      <td>0.808395</td>\n",
       "      <td>0.746119</td>\n",
       "      <td>0.645293</td>\n",
       "      <td>0.794849</td>\n",
       "      <td>0.811017</td>\n",
       "      <td>...</td>\n",
       "      <td>0.641827</td>\n",
       "      <td>0.586026</td>\n",
       "      <td>0.854764</td>\n",
       "      <td>0.623968</td>\n",
       "      <td>0.839451</td>\n",
       "      <td>0.893446</td>\n",
       "      <td>0.635014</td>\n",
       "      <td>0.717671</td>\n",
       "      <td>0.736458</td>\n",
       "      <td>0.817678</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>dress</td>\n",
       "      <td>0.830805</td>\n",
       "      <td>0.784709</td>\n",
       "      <td>0.832871</td>\n",
       "      <td>0.818860</td>\n",
       "      <td>0.754664</td>\n",
       "      <td>0.778052</td>\n",
       "      <td>0.627562</td>\n",
       "      <td>0.820303</td>\n",
       "      <td>0.787593</td>\n",
       "      <td>...</td>\n",
       "      <td>0.583753</td>\n",
       "      <td>0.506032</td>\n",
       "      <td>0.829335</td>\n",
       "      <td>0.539895</td>\n",
       "      <td>0.828975</td>\n",
       "      <td>0.870090</td>\n",
       "      <td>0.535723</td>\n",
       "      <td>0.691407</td>\n",
       "      <td>0.642177</td>\n",
       "      <td>0.749929</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>chair</td>\n",
       "      <td>0.768306</td>\n",
       "      <td>0.672516</td>\n",
       "      <td>0.747876</td>\n",
       "      <td>0.667374</td>\n",
       "      <td>0.600179</td>\n",
       "      <td>0.726622</td>\n",
       "      <td>0.494652</td>\n",
       "      <td>0.716269</td>\n",
       "      <td>0.686106</td>\n",
       "      <td>...</td>\n",
       "      <td>0.506024</td>\n",
       "      <td>0.398247</td>\n",
       "      <td>0.847935</td>\n",
       "      <td>0.454849</td>\n",
       "      <td>0.789435</td>\n",
       "      <td>0.817605</td>\n",
       "      <td>0.583464</td>\n",
       "      <td>0.757182</td>\n",
       "      <td>0.698082</td>\n",
       "      <td>0.690255</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>dresser</td>\n",
       "      <td>0.788659</td>\n",
       "      <td>0.722362</td>\n",
       "      <td>0.595316</td>\n",
       "      <td>0.616838</td>\n",
       "      <td>0.617454</td>\n",
       "      <td>0.481023</td>\n",
       "      <td>0.452728</td>\n",
       "      <td>0.643746</td>\n",
       "      <td>0.639404</td>\n",
       "      <td>...</td>\n",
       "      <td>0.565132</td>\n",
       "      <td>0.487753</td>\n",
       "      <td>0.785302</td>\n",
       "      <td>0.534382</td>\n",
       "      <td>0.770415</td>\n",
       "      <td>0.757685</td>\n",
       "      <td>0.676241</td>\n",
       "      <td>0.781657</td>\n",
       "      <td>0.799060</td>\n",
       "      <td>0.787166</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>bed</td>\n",
       "      <td>0.725328</td>\n",
       "      <td>0.751571</td>\n",
       "      <td>0.598146</td>\n",
       "      <td>0.701655</td>\n",
       "      <td>0.736045</td>\n",
       "      <td>0.547282</td>\n",
       "      <td>0.465465</td>\n",
       "      <td>0.535473</td>\n",
       "      <td>0.619548</td>\n",
       "      <td>...</td>\n",
       "      <td>0.475872</td>\n",
       "      <td>0.483395</td>\n",
       "      <td>0.695193</td>\n",
       "      <td>0.554463</td>\n",
       "      <td>0.728105</td>\n",
       "      <td>0.771604</td>\n",
       "      <td>0.761585</td>\n",
       "      <td>0.770276</td>\n",
       "      <td>0.746601</td>\n",
       "      <td>0.729726</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>table</td>\n",
       "      <td>0.726813</td>\n",
       "      <td>0.676546</td>\n",
       "      <td>0.579006</td>\n",
       "      <td>0.631826</td>\n",
       "      <td>0.659164</td>\n",
       "      <td>0.607550</td>\n",
       "      <td>0.646708</td>\n",
       "      <td>0.617186</td>\n",
       "      <td>0.754221</td>\n",
       "      <td>...</td>\n",
       "      <td>0.428788</td>\n",
       "      <td>0.543479</td>\n",
       "      <td>0.757842</td>\n",
       "      <td>0.608505</td>\n",
       "      <td>0.666310</td>\n",
       "      <td>0.784173</td>\n",
       "      <td>0.713096</td>\n",
       "      <td>0.769848</td>\n",
       "      <td>0.788410</td>\n",
       "      <td>0.780613</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>desk</td>\n",
       "      <td>0.684069</td>\n",
       "      <td>0.627647</td>\n",
       "      <td>0.501265</td>\n",
       "      <td>0.530369</td>\n",
       "      <td>0.567477</td>\n",
       "      <td>0.484901</td>\n",
       "      <td>0.432561</td>\n",
       "      <td>0.496075</td>\n",
       "      <td>0.557414</td>\n",
       "      <td>...</td>\n",
       "      <td>0.384405</td>\n",
       "      <td>0.404495</td>\n",
       "      <td>0.699622</td>\n",
       "      <td>0.484119</td>\n",
       "      <td>0.669863</td>\n",
       "      <td>0.715910</td>\n",
       "      <td>0.739774</td>\n",
       "      <td>0.840375</td>\n",
       "      <td>0.809299</td>\n",
       "      <td>0.768639</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>ant</td>\n",
       "      <td>0.691065</td>\n",
       "      <td>0.777583</td>\n",
       "      <td>0.754469</td>\n",
       "      <td>0.800461</td>\n",
       "      <td>0.834809</td>\n",
       "      <td>0.636324</td>\n",
       "      <td>0.635829</td>\n",
       "      <td>0.691927</td>\n",
       "      <td>0.659346</td>\n",
       "      <td>...</td>\n",
       "      <td>0.411848</td>\n",
       "      <td>0.512513</td>\n",
       "      <td>0.619320</td>\n",
       "      <td>0.554288</td>\n",
       "      <td>0.689799</td>\n",
       "      <td>0.783339</td>\n",
       "      <td>0.598768</td>\n",
       "      <td>0.625851</td>\n",
       "      <td>0.623240</td>\n",
       "      <td>0.722793</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>butterfly</td>\n",
       "      <td>0.738073</td>\n",
       "      <td>0.782226</td>\n",
       "      <td>0.704329</td>\n",
       "      <td>0.776244</td>\n",
       "      <td>0.729092</td>\n",
       "      <td>0.501622</td>\n",
       "      <td>0.300020</td>\n",
       "      <td>0.634290</td>\n",
       "      <td>0.525791</td>\n",
       "      <td>...</td>\n",
       "      <td>0.728920</td>\n",
       "      <td>0.401224</td>\n",
       "      <td>0.703990</td>\n",
       "      <td>0.470192</td>\n",
       "      <td>0.834170</td>\n",
       "      <td>0.722425</td>\n",
       "      <td>0.615877</td>\n",
       "      <td>0.622989</td>\n",
       "      <td>0.567113</td>\n",
       "      <td>0.688224</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>fly</td>\n",
       "      <td>0.870237</td>\n",
       "      <td>0.852732</td>\n",
       "      <td>0.799885</td>\n",
       "      <td>0.828591</td>\n",
       "      <td>0.807930</td>\n",
       "      <td>0.631929</td>\n",
       "      <td>0.632948</td>\n",
       "      <td>0.802939</td>\n",
       "      <td>0.762990</td>\n",
       "      <td>...</td>\n",
       "      <td>0.607168</td>\n",
       "      <td>0.572116</td>\n",
       "      <td>0.743052</td>\n",
       "      <td>0.604350</td>\n",
       "      <td>0.785248</td>\n",
       "      <td>0.849898</td>\n",
       "      <td>0.539665</td>\n",
       "      <td>0.640618</td>\n",
       "      <td>0.728240</td>\n",
       "      <td>0.822813</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>bee</td>\n",
       "      <td>0.902443</td>\n",
       "      <td>0.890533</td>\n",
       "      <td>0.875222</td>\n",
       "      <td>0.878088</td>\n",
       "      <td>0.826075</td>\n",
       "      <td>0.736508</td>\n",
       "      <td>0.567510</td>\n",
       "      <td>0.819981</td>\n",
       "      <td>0.758506</td>\n",
       "      <td>...</td>\n",
       "      <td>0.677455</td>\n",
       "      <td>0.506895</td>\n",
       "      <td>0.844244</td>\n",
       "      <td>0.581631</td>\n",
       "      <td>0.852035</td>\n",
       "      <td>0.908592</td>\n",
       "      <td>0.646222</td>\n",
       "      <td>0.737953</td>\n",
       "      <td>0.734187</td>\n",
       "      <td>0.818343</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>beetle</td>\n",
       "      <td>0.857445</td>\n",
       "      <td>0.866394</td>\n",
       "      <td>0.785367</td>\n",
       "      <td>0.859938</td>\n",
       "      <td>0.809364</td>\n",
       "      <td>0.643236</td>\n",
       "      <td>0.531996</td>\n",
       "      <td>0.761997</td>\n",
       "      <td>0.725093</td>\n",
       "      <td>...</td>\n",
       "      <td>0.669382</td>\n",
       "      <td>0.562920</td>\n",
       "      <td>0.776933</td>\n",
       "      <td>0.635357</td>\n",
       "      <td>0.844725</td>\n",
       "      <td>0.859050</td>\n",
       "      <td>0.635763</td>\n",
       "      <td>0.675059</td>\n",
       "      <td>0.692186</td>\n",
       "      <td>0.793873</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>knife</td>\n",
       "      <td>0.587953</td>\n",
       "      <td>0.616830</td>\n",
       "      <td>0.514735</td>\n",
       "      <td>0.576219</td>\n",
       "      <td>0.615567</td>\n",
       "      <td>0.544385</td>\n",
       "      <td>0.702187</td>\n",
       "      <td>0.568099</td>\n",
       "      <td>0.705813</td>\n",
       "      <td>...</td>\n",
       "      <td>0.413247</td>\n",
       "      <td>0.654923</td>\n",
       "      <td>0.611017</td>\n",
       "      <td>0.753891</td>\n",
       "      <td>0.540646</td>\n",
       "      <td>0.698375</td>\n",
       "      <td>0.509632</td>\n",
       "      <td>0.549013</td>\n",
       "      <td>0.565088</td>\n",
       "      <td>0.624987</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>bottle</td>\n",
       "      <td>0.695385</td>\n",
       "      <td>0.644273</td>\n",
       "      <td>0.758616</td>\n",
       "      <td>0.664466</td>\n",
       "      <td>0.583204</td>\n",
       "      <td>0.768683</td>\n",
       "      <td>0.539871</td>\n",
       "      <td>0.715812</td>\n",
       "      <td>0.694144</td>\n",
       "      <td>...</td>\n",
       "      <td>0.493462</td>\n",
       "      <td>0.429526</td>\n",
       "      <td>0.832812</td>\n",
       "      <td>0.440403</td>\n",
       "      <td>0.768282</td>\n",
       "      <td>0.786941</td>\n",
       "      <td>0.502812</td>\n",
       "      <td>0.728353</td>\n",
       "      <td>0.628853</td>\n",
       "      <td>0.654319</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>glass</td>\n",
       "      <td>0.669068</td>\n",
       "      <td>0.609779</td>\n",
       "      <td>0.716930</td>\n",
       "      <td>0.602703</td>\n",
       "      <td>0.465573</td>\n",
       "      <td>0.628759</td>\n",
       "      <td>0.303552</td>\n",
       "      <td>0.672058</td>\n",
       "      <td>0.564443</td>\n",
       "      <td>...</td>\n",
       "      <td>0.618897</td>\n",
       "      <td>0.352579</td>\n",
       "      <td>0.770443</td>\n",
       "      <td>0.323041</td>\n",
       "      <td>0.776409</td>\n",
       "      <td>0.685610</td>\n",
       "      <td>0.410371</td>\n",
       "      <td>0.611967</td>\n",
       "      <td>0.541319</td>\n",
       "      <td>0.559989</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>spoon</td>\n",
       "      <td>0.689416</td>\n",
       "      <td>0.703103</td>\n",
       "      <td>0.557828</td>\n",
       "      <td>0.682571</td>\n",
       "      <td>0.697471</td>\n",
       "      <td>0.555059</td>\n",
       "      <td>0.722344</td>\n",
       "      <td>0.649860</td>\n",
       "      <td>0.789202</td>\n",
       "      <td>...</td>\n",
       "      <td>0.513288</td>\n",
       "      <td>0.699333</td>\n",
       "      <td>0.613033</td>\n",
       "      <td>0.820611</td>\n",
       "      <td>0.604297</td>\n",
       "      <td>0.763830</td>\n",
       "      <td>0.599706</td>\n",
       "      <td>0.562963</td>\n",
       "      <td>0.663477</td>\n",
       "      <td>0.706858</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>cup</td>\n",
       "      <td>0.631058</td>\n",
       "      <td>0.672261</td>\n",
       "      <td>0.557587</td>\n",
       "      <td>0.678326</td>\n",
       "      <td>0.642021</td>\n",
       "      <td>0.523357</td>\n",
       "      <td>0.439673</td>\n",
       "      <td>0.570131</td>\n",
       "      <td>0.596660</td>\n",
       "      <td>...</td>\n",
       "      <td>0.680988</td>\n",
       "      <td>0.474785</td>\n",
       "      <td>0.656271</td>\n",
       "      <td>0.595870</td>\n",
       "      <td>0.635976</td>\n",
       "      <td>0.674283</td>\n",
       "      <td>0.683660</td>\n",
       "      <td>0.596926</td>\n",
       "      <td>0.604378</td>\n",
       "      <td>0.625303</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>telephone</td>\n",
       "      <td>0.907903</td>\n",
       "      <td>0.809121</td>\n",
       "      <td>0.770225</td>\n",
       "      <td>0.807333</td>\n",
       "      <td>0.764701</td>\n",
       "      <td>0.724612</td>\n",
       "      <td>0.574842</td>\n",
       "      <td>0.772525</td>\n",
       "      <td>0.773986</td>\n",
       "      <td>...</td>\n",
       "      <td>0.645410</td>\n",
       "      <td>0.546342</td>\n",
       "      <td>0.818373</td>\n",
       "      <td>0.585782</td>\n",
       "      <td>0.810940</td>\n",
       "      <td>0.890905</td>\n",
       "      <td>0.608256</td>\n",
       "      <td>0.671767</td>\n",
       "      <td>0.714214</td>\n",
       "      <td>0.789244</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>key</td>\n",
       "      <td>0.697340</td>\n",
       "      <td>0.703172</td>\n",
       "      <td>0.850677</td>\n",
       "      <td>0.734032</td>\n",
       "      <td>0.635849</td>\n",
       "      <td>0.761813</td>\n",
       "      <td>0.451965</td>\n",
       "      <td>0.772869</td>\n",
       "      <td>0.624108</td>\n",
       "      <td>...</td>\n",
       "      <td>0.563528</td>\n",
       "      <td>0.350159</td>\n",
       "      <td>0.824987</td>\n",
       "      <td>0.384281</td>\n",
       "      <td>0.794073</td>\n",
       "      <td>0.771416</td>\n",
       "      <td>0.495346</td>\n",
       "      <td>0.652303</td>\n",
       "      <td>0.568583</td>\n",
       "      <td>0.642873</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>watch</td>\n",
       "      <td>0.756502</td>\n",
       "      <td>0.698777</td>\n",
       "      <td>0.678881</td>\n",
       "      <td>0.727094</td>\n",
       "      <td>0.746676</td>\n",
       "      <td>0.739074</td>\n",
       "      <td>0.644681</td>\n",
       "      <td>0.672264</td>\n",
       "      <td>0.781115</td>\n",
       "      <td>...</td>\n",
       "      <td>0.424524</td>\n",
       "      <td>0.554331</td>\n",
       "      <td>0.811393</td>\n",
       "      <td>0.617313</td>\n",
       "      <td>0.738202</td>\n",
       "      <td>0.847350</td>\n",
       "      <td>0.704555</td>\n",
       "      <td>0.733981</td>\n",
       "      <td>0.741210</td>\n",
       "      <td>0.751133</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>refrigerator</td>\n",
       "      <td>0.745210</td>\n",
       "      <td>0.726670</td>\n",
       "      <td>0.698859</td>\n",
       "      <td>0.658273</td>\n",
       "      <td>0.641891</td>\n",
       "      <td>0.618369</td>\n",
       "      <td>0.334180</td>\n",
       "      <td>0.556853</td>\n",
       "      <td>0.545528</td>\n",
       "      <td>...</td>\n",
       "      <td>0.590699</td>\n",
       "      <td>0.427098</td>\n",
       "      <td>0.799737</td>\n",
       "      <td>0.481697</td>\n",
       "      <td>0.798355</td>\n",
       "      <td>0.752779</td>\n",
       "      <td>0.663390</td>\n",
       "      <td>0.795322</td>\n",
       "      <td>0.687058</td>\n",
       "      <td>0.718935</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>bell</td>\n",
       "      <td>0.761587</td>\n",
       "      <td>0.770673</td>\n",
       "      <td>0.752112</td>\n",
       "      <td>0.704024</td>\n",
       "      <td>0.705145</td>\n",
       "      <td>0.596411</td>\n",
       "      <td>0.459060</td>\n",
       "      <td>0.664241</td>\n",
       "      <td>0.624405</td>\n",
       "      <td>...</td>\n",
       "      <td>0.633726</td>\n",
       "      <td>0.540109</td>\n",
       "      <td>0.809442</td>\n",
       "      <td>0.577536</td>\n",
       "      <td>0.854777</td>\n",
       "      <td>0.793306</td>\n",
       "      <td>0.618322</td>\n",
       "      <td>0.750792</td>\n",
       "      <td>0.703394</td>\n",
       "      <td>0.799112</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>screwdriver</td>\n",
       "      <td>0.525716</td>\n",
       "      <td>0.512598</td>\n",
       "      <td>0.416282</td>\n",
       "      <td>0.524439</td>\n",
       "      <td>0.564325</td>\n",
       "      <td>0.562187</td>\n",
       "      <td>0.744482</td>\n",
       "      <td>0.526077</td>\n",
       "      <td>0.747239</td>\n",
       "      <td>...</td>\n",
       "      <td>0.276495</td>\n",
       "      <td>0.665802</td>\n",
       "      <td>0.544270</td>\n",
       "      <td>0.737703</td>\n",
       "      <td>0.457360</td>\n",
       "      <td>0.651132</td>\n",
       "      <td>0.429106</td>\n",
       "      <td>0.427948</td>\n",
       "      <td>0.497080</td>\n",
       "      <td>0.541481</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>chisel</td>\n",
       "      <td>0.837904</td>\n",
       "      <td>0.714995</td>\n",
       "      <td>0.777330</td>\n",
       "      <td>0.718992</td>\n",
       "      <td>0.638073</td>\n",
       "      <td>0.789969</td>\n",
       "      <td>0.624913</td>\n",
       "      <td>0.791892</td>\n",
       "      <td>0.802794</td>\n",
       "      <td>...</td>\n",
       "      <td>0.569381</td>\n",
       "      <td>0.479680</td>\n",
       "      <td>0.851834</td>\n",
       "      <td>0.553871</td>\n",
       "      <td>0.740476</td>\n",
       "      <td>0.862722</td>\n",
       "      <td>0.499067</td>\n",
       "      <td>0.655145</td>\n",
       "      <td>0.669871</td>\n",
       "      <td>0.713600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>pliers</td>\n",
       "      <td>0.662072</td>\n",
       "      <td>0.683640</td>\n",
       "      <td>0.563572</td>\n",
       "      <td>0.703940</td>\n",
       "      <td>0.724202</td>\n",
       "      <td>0.587292</td>\n",
       "      <td>0.602502</td>\n",
       "      <td>0.586857</td>\n",
       "      <td>0.710188</td>\n",
       "      <td>...</td>\n",
       "      <td>0.526958</td>\n",
       "      <td>0.667040</td>\n",
       "      <td>0.687934</td>\n",
       "      <td>0.653829</td>\n",
       "      <td>0.670162</td>\n",
       "      <td>0.721488</td>\n",
       "      <td>0.545915</td>\n",
       "      <td>0.517871</td>\n",
       "      <td>0.557607</td>\n",
       "      <td>0.633766</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>saw</td>\n",
       "      <td>0.638146</td>\n",
       "      <td>0.679344</td>\n",
       "      <td>0.538485</td>\n",
       "      <td>0.648948</td>\n",
       "      <td>0.691530</td>\n",
       "      <td>0.543095</td>\n",
       "      <td>0.712552</td>\n",
       "      <td>0.576657</td>\n",
       "      <td>0.746569</td>\n",
       "      <td>...</td>\n",
       "      <td>0.490540</td>\n",
       "      <td>0.650495</td>\n",
       "      <td>0.628111</td>\n",
       "      <td>0.784718</td>\n",
       "      <td>0.575359</td>\n",
       "      <td>0.731213</td>\n",
       "      <td>0.642383</td>\n",
       "      <td>0.620108</td>\n",
       "      <td>0.665620</td>\n",
       "      <td>0.715041</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>hammer</td>\n",
       "      <td>0.556451</td>\n",
       "      <td>0.545172</td>\n",
       "      <td>0.433060</td>\n",
       "      <td>0.577171</td>\n",
       "      <td>0.636218</td>\n",
       "      <td>0.524448</td>\n",
       "      <td>0.717967</td>\n",
       "      <td>0.522666</td>\n",
       "      <td>0.717579</td>\n",
       "      <td>...</td>\n",
       "      <td>0.360475</td>\n",
       "      <td>0.708623</td>\n",
       "      <td>0.579661</td>\n",
       "      <td>0.769853</td>\n",
       "      <td>0.512744</td>\n",
       "      <td>0.657830</td>\n",
       "      <td>0.538586</td>\n",
       "      <td>0.490798</td>\n",
       "      <td>0.572200</td>\n",
       "      <td>0.603538</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50</th>\n",
       "      <td>tomato</td>\n",
       "      <td>0.679341</td>\n",
       "      <td>0.678013</td>\n",
       "      <td>0.571532</td>\n",
       "      <td>0.644053</td>\n",
       "      <td>0.561507</td>\n",
       "      <td>0.324236</td>\n",
       "      <td>0.211426</td>\n",
       "      <td>0.528932</td>\n",
       "      <td>0.449388</td>\n",
       "      <td>...</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.500313</td>\n",
       "      <td>0.561782</td>\n",
       "      <td>0.506339</td>\n",
       "      <td>0.678939</td>\n",
       "      <td>0.560498</td>\n",
       "      <td>0.386559</td>\n",
       "      <td>0.380307</td>\n",
       "      <td>0.442160</td>\n",
       "      <td>0.541360</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51</th>\n",
       "      <td>celery</td>\n",
       "      <td>0.517919</td>\n",
       "      <td>0.514908</td>\n",
       "      <td>0.317849</td>\n",
       "      <td>0.475084</td>\n",
       "      <td>0.544593</td>\n",
       "      <td>0.241527</td>\n",
       "      <td>0.550446</td>\n",
       "      <td>0.426071</td>\n",
       "      <td>0.578857</td>\n",
       "      <td>...</td>\n",
       "      <td>0.500313</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.475159</td>\n",
       "      <td>0.763209</td>\n",
       "      <td>0.542782</td>\n",
       "      <td>0.519049</td>\n",
       "      <td>0.344661</td>\n",
       "      <td>0.334831</td>\n",
       "      <td>0.504676</td>\n",
       "      <td>0.582738</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>52</th>\n",
       "      <td>corn</td>\n",
       "      <td>0.809874</td>\n",
       "      <td>0.710625</td>\n",
       "      <td>0.771336</td>\n",
       "      <td>0.698746</td>\n",
       "      <td>0.655788</td>\n",
       "      <td>0.740154</td>\n",
       "      <td>0.497993</td>\n",
       "      <td>0.744326</td>\n",
       "      <td>0.695290</td>\n",
       "      <td>...</td>\n",
       "      <td>0.561782</td>\n",
       "      <td>0.475159</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.456373</td>\n",
       "      <td>0.824888</td>\n",
       "      <td>0.818197</td>\n",
       "      <td>0.574108</td>\n",
       "      <td>0.708325</td>\n",
       "      <td>0.667350</td>\n",
       "      <td>0.699529</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53</th>\n",
       "      <td>carrot</td>\n",
       "      <td>0.586457</td>\n",
       "      <td>0.626575</td>\n",
       "      <td>0.417673</td>\n",
       "      <td>0.537472</td>\n",
       "      <td>0.623611</td>\n",
       "      <td>0.298281</td>\n",
       "      <td>0.652083</td>\n",
       "      <td>0.466841</td>\n",
       "      <td>0.649962</td>\n",
       "      <td>...</td>\n",
       "      <td>0.506339</td>\n",
       "      <td>0.763209</td>\n",
       "      <td>0.456373</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.498010</td>\n",
       "      <td>0.624687</td>\n",
       "      <td>0.554217</td>\n",
       "      <td>0.499876</td>\n",
       "      <td>0.660626</td>\n",
       "      <td>0.713950</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>54</th>\n",
       "      <td>lettuce</td>\n",
       "      <td>0.814620</td>\n",
       "      <td>0.799161</td>\n",
       "      <td>0.768428</td>\n",
       "      <td>0.784360</td>\n",
       "      <td>0.740062</td>\n",
       "      <td>0.635136</td>\n",
       "      <td>0.436008</td>\n",
       "      <td>0.739569</td>\n",
       "      <td>0.644539</td>\n",
       "      <td>...</td>\n",
       "      <td>0.678939</td>\n",
       "      <td>0.542782</td>\n",
       "      <td>0.824888</td>\n",
       "      <td>0.498010</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.814918</td>\n",
       "      <td>0.565864</td>\n",
       "      <td>0.692537</td>\n",
       "      <td>0.664510</td>\n",
       "      <td>0.742056</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>55</th>\n",
       "      <td>bicycle</td>\n",
       "      <td>0.886597</td>\n",
       "      <td>0.849823</td>\n",
       "      <td>0.835674</td>\n",
       "      <td>0.831094</td>\n",
       "      <td>0.792543</td>\n",
       "      <td>0.779629</td>\n",
       "      <td>0.698630</td>\n",
       "      <td>0.838479</td>\n",
       "      <td>0.824975</td>\n",
       "      <td>...</td>\n",
       "      <td>0.560498</td>\n",
       "      <td>0.519049</td>\n",
       "      <td>0.818197</td>\n",
       "      <td>0.624687</td>\n",
       "      <td>0.814918</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.655187</td>\n",
       "      <td>0.790419</td>\n",
       "      <td>0.787582</td>\n",
       "      <td>0.843646</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>56</th>\n",
       "      <td>car</td>\n",
       "      <td>0.591752</td>\n",
       "      <td>0.696179</td>\n",
       "      <td>0.513325</td>\n",
       "      <td>0.610807</td>\n",
       "      <td>0.700272</td>\n",
       "      <td>0.405805</td>\n",
       "      <td>0.389628</td>\n",
       "      <td>0.416598</td>\n",
       "      <td>0.479248</td>\n",
       "      <td>...</td>\n",
       "      <td>0.386559</td>\n",
       "      <td>0.344661</td>\n",
       "      <td>0.574108</td>\n",
       "      <td>0.554217</td>\n",
       "      <td>0.565864</td>\n",
       "      <td>0.655187</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.802225</td>\n",
       "      <td>0.798727</td>\n",
       "      <td>0.745591</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>57</th>\n",
       "      <td>train</td>\n",
       "      <td>0.698553</td>\n",
       "      <td>0.748991</td>\n",
       "      <td>0.669761</td>\n",
       "      <td>0.618325</td>\n",
       "      <td>0.650916</td>\n",
       "      <td>0.586700</td>\n",
       "      <td>0.465599</td>\n",
       "      <td>0.552607</td>\n",
       "      <td>0.553614</td>\n",
       "      <td>...</td>\n",
       "      <td>0.380307</td>\n",
       "      <td>0.334831</td>\n",
       "      <td>0.708325</td>\n",
       "      <td>0.499876</td>\n",
       "      <td>0.692537</td>\n",
       "      <td>0.790419</td>\n",
       "      <td>0.802225</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.829152</td>\n",
       "      <td>0.822606</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>58</th>\n",
       "      <td>truck</td>\n",
       "      <td>0.770612</td>\n",
       "      <td>0.761983</td>\n",
       "      <td>0.608832</td>\n",
       "      <td>0.649231</td>\n",
       "      <td>0.690289</td>\n",
       "      <td>0.483917</td>\n",
       "      <td>0.566975</td>\n",
       "      <td>0.609400</td>\n",
       "      <td>0.694302</td>\n",
       "      <td>...</td>\n",
       "      <td>0.442160</td>\n",
       "      <td>0.504676</td>\n",
       "      <td>0.667350</td>\n",
       "      <td>0.660626</td>\n",
       "      <td>0.664510</td>\n",
       "      <td>0.787582</td>\n",
       "      <td>0.798727</td>\n",
       "      <td>0.829152</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.880917</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59</th>\n",
       "      <td>airplane</td>\n",
       "      <td>0.830323</td>\n",
       "      <td>0.828290</td>\n",
       "      <td>0.677270</td>\n",
       "      <td>0.724007</td>\n",
       "      <td>0.769298</td>\n",
       "      <td>0.521596</td>\n",
       "      <td>0.611928</td>\n",
       "      <td>0.669398</td>\n",
       "      <td>0.709507</td>\n",
       "      <td>...</td>\n",
       "      <td>0.541360</td>\n",
       "      <td>0.582738</td>\n",
       "      <td>0.699529</td>\n",
       "      <td>0.713950</td>\n",
       "      <td>0.742056</td>\n",
       "      <td>0.843646</td>\n",
       "      <td>0.745591</td>\n",
       "      <td>0.822606</td>\n",
       "      <td>0.880917</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>60 rows × 61 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            term     horse      bear       cat       dog       cow       leg  \\\n",
       "0          horse  1.000000  0.867668  0.794402  0.817715  0.762252  0.655147   \n",
       "1           bear  0.867668  1.000000  0.784209  0.856424  0.842841  0.576497   \n",
       "2            cat  0.794402  0.784209  1.000000  0.801168  0.706735  0.798449   \n",
       "3            dog  0.817715  0.856424  0.801168  1.000000  0.869596  0.669203   \n",
       "4            cow  0.762252  0.842841  0.706735  0.869596  1.000000  0.564517   \n",
       "5            leg  0.655147  0.576497  0.798449  0.669203  0.564517  1.000000   \n",
       "6            arm  0.560490  0.529558  0.518258  0.587157  0.602313  0.589304   \n",
       "7           hand  0.798085  0.703956  0.803878  0.763389  0.636055  0.745915   \n",
       "8           foot  0.773818  0.695968  0.672096  0.744822  0.691362  0.706871   \n",
       "9            eye  0.811312  0.797959  0.739001  0.761645  0.764978  0.678523   \n",
       "10     apartment  0.698721  0.762514  0.596658  0.621620  0.608525  0.445907   \n",
       "11         igloo  0.661859  0.688277  0.670116  0.640349  0.679977  0.570472   \n",
       "12          barn  0.711925  0.737495  0.622361  0.619353  0.651241  0.470127   \n",
       "13         house  0.635954  0.703997  0.529147  0.588550  0.651021  0.356240   \n",
       "14        church  0.632664  0.686622  0.568706  0.601965  0.621611  0.481075   \n",
       "15          door  0.664959  0.697948  0.707445  0.654743  0.614215  0.660156   \n",
       "16        closet  0.715603  0.687266  0.673670  0.612249  0.605650  0.643443   \n",
       "17       chimney  0.758883  0.759291  0.688216  0.698518  0.717023  0.673674   \n",
       "18        window  0.762504  0.718538  0.666203  0.622478  0.579022  0.610803   \n",
       "19          arch  0.610093  0.671840  0.491565  0.505957  0.506032  0.395335   \n",
       "20         shirt  0.878576  0.834256  0.813089  0.840423  0.777312  0.678780   \n",
       "21         pants  0.777598  0.736485  0.794516  0.776590  0.731372  0.805345   \n",
       "22         skirt  0.822725  0.790998  0.769078  0.799265  0.749963  0.688187   \n",
       "23          coat  0.889299  0.816334  0.796006  0.839576  0.808395  0.746119   \n",
       "24         dress  0.830805  0.784709  0.832871  0.818860  0.754664  0.778052   \n",
       "25         chair  0.768306  0.672516  0.747876  0.667374  0.600179  0.726622   \n",
       "26       dresser  0.788659  0.722362  0.595316  0.616838  0.617454  0.481023   \n",
       "27           bed  0.725328  0.751571  0.598146  0.701655  0.736045  0.547282   \n",
       "28         table  0.726813  0.676546  0.579006  0.631826  0.659164  0.607550   \n",
       "29          desk  0.684069  0.627647  0.501265  0.530369  0.567477  0.484901   \n",
       "30           ant  0.691065  0.777583  0.754469  0.800461  0.834809  0.636324   \n",
       "31     butterfly  0.738073  0.782226  0.704329  0.776244  0.729092  0.501622   \n",
       "32           fly  0.870237  0.852732  0.799885  0.828591  0.807930  0.631929   \n",
       "33           bee  0.902443  0.890533  0.875222  0.878088  0.826075  0.736508   \n",
       "34        beetle  0.857445  0.866394  0.785367  0.859938  0.809364  0.643236   \n",
       "35         knife  0.587953  0.616830  0.514735  0.576219  0.615567  0.544385   \n",
       "36        bottle  0.695385  0.644273  0.758616  0.664466  0.583204  0.768683   \n",
       "37         glass  0.669068  0.609779  0.716930  0.602703  0.465573  0.628759   \n",
       "38         spoon  0.689416  0.703103  0.557828  0.682571  0.697471  0.555059   \n",
       "39           cup  0.631058  0.672261  0.557587  0.678326  0.642021  0.523357   \n",
       "40     telephone  0.907903  0.809121  0.770225  0.807333  0.764701  0.724612   \n",
       "41           key  0.697340  0.703172  0.850677  0.734032  0.635849  0.761813   \n",
       "42         watch  0.756502  0.698777  0.678881  0.727094  0.746676  0.739074   \n",
       "43  refrigerator  0.745210  0.726670  0.698859  0.658273  0.641891  0.618369   \n",
       "44          bell  0.761587  0.770673  0.752112  0.704024  0.705145  0.596411   \n",
       "45   screwdriver  0.525716  0.512598  0.416282  0.524439  0.564325  0.562187   \n",
       "46        chisel  0.837904  0.714995  0.777330  0.718992  0.638073  0.789969   \n",
       "47        pliers  0.662072  0.683640  0.563572  0.703940  0.724202  0.587292   \n",
       "48           saw  0.638146  0.679344  0.538485  0.648948  0.691530  0.543095   \n",
       "49        hammer  0.556451  0.545172  0.433060  0.577171  0.636218  0.524448   \n",
       "50        tomato  0.679341  0.678013  0.571532  0.644053  0.561507  0.324236   \n",
       "51        celery  0.517919  0.514908  0.317849  0.475084  0.544593  0.241527   \n",
       "52          corn  0.809874  0.710625  0.771336  0.698746  0.655788  0.740154   \n",
       "53        carrot  0.586457  0.626575  0.417673  0.537472  0.623611  0.298281   \n",
       "54       lettuce  0.814620  0.799161  0.768428  0.784360  0.740062  0.635136   \n",
       "55       bicycle  0.886597  0.849823  0.835674  0.831094  0.792543  0.779629   \n",
       "56           car  0.591752  0.696179  0.513325  0.610807  0.700272  0.405805   \n",
       "57         train  0.698553  0.748991  0.669761  0.618325  0.650916  0.586700   \n",
       "58         truck  0.770612  0.761983  0.608832  0.649231  0.690289  0.483917   \n",
       "59      airplane  0.830323  0.828290  0.677270  0.724007  0.769298  0.521596   \n",
       "\n",
       "         arm      hand      foot  ...    tomato    celery      corn    carrot  \\\n",
       "0   0.560490  0.798085  0.773818  ...  0.679341  0.517919  0.809874  0.586457   \n",
       "1   0.529558  0.703956  0.695968  ...  0.678013  0.514908  0.710625  0.626575   \n",
       "2   0.518258  0.803878  0.672096  ...  0.571532  0.317849  0.771336  0.417673   \n",
       "3   0.587157  0.763389  0.744822  ...  0.644053  0.475084  0.698746  0.537472   \n",
       "4   0.602313  0.636055  0.691362  ...  0.561507  0.544593  0.655788  0.623611   \n",
       "5   0.589304  0.745915  0.706871  ...  0.324236  0.241527  0.740154  0.298281   \n",
       "6   1.000000  0.683080  0.846284  ...  0.211426  0.550446  0.497993  0.652083   \n",
       "7   0.683080  1.000000  0.840091  ...  0.528932  0.426071  0.744326  0.466841   \n",
       "8   0.846284  0.840091  1.000000  ...  0.449388  0.578857  0.695290  0.649962   \n",
       "9   0.614393  0.756584  0.762380  ...  0.518579  0.535730  0.842577  0.609926   \n",
       "10  0.231247  0.482260  0.448296  ...  0.587824  0.295193  0.649879  0.399071   \n",
       "11  0.403760  0.536716  0.530904  ...  0.415552  0.403774  0.759893  0.475588   \n",
       "12  0.356480  0.513918  0.526572  ...  0.560346  0.375587  0.661230  0.501225   \n",
       "13  0.172805  0.388721  0.360437  ...  0.512642  0.301658  0.615431  0.407944   \n",
       "14  0.309655  0.449913  0.464908  ...  0.460718  0.367576  0.673649  0.421551   \n",
       "15  0.389011  0.605931  0.573216  ...  0.484830  0.277677  0.765617  0.370305   \n",
       "16  0.464889  0.615073  0.621563  ...  0.392241  0.312676  0.751869  0.463871   \n",
       "17  0.513790  0.589217  0.669381  ...  0.451119  0.457083  0.772736  0.514505   \n",
       "18  0.376803  0.623167  0.589509  ...  0.586896  0.428371  0.784558  0.458079   \n",
       "19  0.280516  0.423684  0.435040  ...  0.525820  0.316577  0.620526  0.438808   \n",
       "20  0.494433  0.778000  0.727838  ...  0.741920  0.529024  0.819480  0.538137   \n",
       "21  0.596718  0.750308  0.754078  ...  0.492070  0.477946  0.858527  0.495714   \n",
       "22  0.486777  0.759455  0.678832  ...  0.587417  0.492523  0.792462  0.491346   \n",
       "23  0.645293  0.794849  0.811017  ...  0.641827  0.586026  0.854764  0.623968   \n",
       "24  0.627562  0.820303  0.787593  ...  0.583753  0.506032  0.829335  0.539895   \n",
       "25  0.494652  0.716269  0.686106  ...  0.506024  0.398247  0.847935  0.454849   \n",
       "26  0.452728  0.643746  0.639404  ...  0.565132  0.487753  0.785302  0.534382   \n",
       "27  0.465465  0.535473  0.619548  ...  0.475872  0.483395  0.695193  0.554463   \n",
       "28  0.646708  0.617186  0.754221  ...  0.428788  0.543479  0.757842  0.608505   \n",
       "29  0.432561  0.496075  0.557414  ...  0.384405  0.404495  0.699622  0.484119   \n",
       "30  0.635829  0.691927  0.659346  ...  0.411848  0.512513  0.619320  0.554288   \n",
       "31  0.300020  0.634290  0.525791  ...  0.728920  0.401224  0.703990  0.470192   \n",
       "32  0.632948  0.802939  0.762990  ...  0.607168  0.572116  0.743052  0.604350   \n",
       "33  0.567510  0.819981  0.758506  ...  0.677455  0.506895  0.844244  0.581631   \n",
       "34  0.531996  0.761997  0.725093  ...  0.669382  0.562920  0.776933  0.635357   \n",
       "35  0.702187  0.568099  0.705813  ...  0.413247  0.654923  0.611017  0.753891   \n",
       "36  0.539871  0.715812  0.694144  ...  0.493462  0.429526  0.832812  0.440403   \n",
       "37  0.303552  0.672058  0.564443  ...  0.618897  0.352579  0.770443  0.323041   \n",
       "38  0.722344  0.649860  0.789202  ...  0.513288  0.699333  0.613033  0.820611   \n",
       "39  0.439673  0.570131  0.596660  ...  0.680988  0.474785  0.656271  0.595870   \n",
       "40  0.574842  0.772525  0.773986  ...  0.645410  0.546342  0.818373  0.585782   \n",
       "41  0.451965  0.772869  0.624108  ...  0.563528  0.350159  0.824987  0.384281   \n",
       "42  0.644681  0.672264  0.781115  ...  0.424524  0.554331  0.811393  0.617313   \n",
       "43  0.334180  0.556853  0.545528  ...  0.590699  0.427098  0.799737  0.481697   \n",
       "44  0.459060  0.664241  0.624405  ...  0.633726  0.540109  0.809442  0.577536   \n",
       "45  0.744482  0.526077  0.747239  ...  0.276495  0.665802  0.544270  0.737703   \n",
       "46  0.624913  0.791892  0.802794  ...  0.569381  0.479680  0.851834  0.553871   \n",
       "47  0.602502  0.586857  0.710188  ...  0.526958  0.667040  0.687934  0.653829   \n",
       "48  0.712552  0.576657  0.746569  ...  0.490540  0.650495  0.628111  0.784718   \n",
       "49  0.717967  0.522666  0.717579  ...  0.360475  0.708623  0.579661  0.769853   \n",
       "50  0.211426  0.528932  0.449388  ...  1.000000  0.500313  0.561782  0.506339   \n",
       "51  0.550446  0.426071  0.578857  ...  0.500313  1.000000  0.475159  0.763209   \n",
       "52  0.497993  0.744326  0.695290  ...  0.561782  0.475159  1.000000  0.456373   \n",
       "53  0.652083  0.466841  0.649962  ...  0.506339  0.763209  0.456373  1.000000   \n",
       "54  0.436008  0.739569  0.644539  ...  0.678939  0.542782  0.824888  0.498010   \n",
       "55  0.698630  0.838479  0.824975  ...  0.560498  0.519049  0.818197  0.624687   \n",
       "56  0.389628  0.416598  0.479248  ...  0.386559  0.344661  0.574108  0.554217   \n",
       "57  0.465599  0.552607  0.553614  ...  0.380307  0.334831  0.708325  0.499876   \n",
       "58  0.566975  0.609400  0.694302  ...  0.442160  0.504676  0.667350  0.660626   \n",
       "59  0.611928  0.669398  0.709507  ...  0.541360  0.582738  0.699529  0.713950   \n",
       "\n",
       "     lettuce   bicycle       car     train     truck  airplane  \n",
       "0   0.814620  0.886597  0.591752  0.698553  0.770612  0.830323  \n",
       "1   0.799161  0.849823  0.696179  0.748991  0.761983  0.828290  \n",
       "2   0.768428  0.835674  0.513325  0.669761  0.608832  0.677270  \n",
       "3   0.784360  0.831094  0.610807  0.618325  0.649231  0.724007  \n",
       "4   0.740062  0.792543  0.700272  0.650916  0.690289  0.769298  \n",
       "5   0.635136  0.779629  0.405805  0.586700  0.483917  0.521596  \n",
       "6   0.436008  0.698630  0.389628  0.465599  0.566975  0.611928  \n",
       "7   0.739569  0.838479  0.416598  0.552607  0.609400  0.669398  \n",
       "8   0.644539  0.824975  0.479248  0.553614  0.694302  0.709507  \n",
       "9   0.822865  0.888571  0.742480  0.823249  0.822879  0.826608  \n",
       "10  0.730971  0.689024  0.722454  0.822750  0.710393  0.733315  \n",
       "11  0.739594  0.736073  0.753110  0.824186  0.746618  0.724672  \n",
       "12  0.706021  0.714799  0.758685  0.815631  0.817320  0.801912  \n",
       "13  0.699130  0.633268  0.788316  0.815476  0.734614  0.730749  \n",
       "14  0.727649  0.695612  0.714085  0.826501  0.705087  0.706253  \n",
       "15  0.743025  0.744535  0.694549  0.837039  0.686668  0.699127  \n",
       "16  0.706825  0.801271  0.740680  0.883244  0.787451  0.748007  \n",
       "17  0.763782  0.831779  0.743953  0.859954  0.767059  0.789290  \n",
       "18  0.786179  0.769819  0.670236  0.804625  0.735202  0.742122  \n",
       "19  0.636221  0.630249  0.692751  0.800162  0.693101  0.709135  \n",
       "20  0.886150  0.848812  0.582945  0.659794  0.675980  0.766515  \n",
       "21  0.809120  0.848815  0.570421  0.724547  0.629747  0.704129  \n",
       "22  0.854875  0.833014  0.564682  0.670851  0.626155  0.729970  \n",
       "23  0.839451  0.893446  0.635014  0.717671  0.736458  0.817678  \n",
       "24  0.828975  0.870090  0.535723  0.691407  0.642177  0.749929  \n",
       "25  0.789435  0.817605  0.583464  0.757182  0.698082  0.690255  \n",
       "26  0.770415  0.757685  0.676241  0.781657  0.799060  0.787166  \n",
       "27  0.728105  0.771604  0.761585  0.770276  0.746601  0.729726  \n",
       "28  0.666310  0.784173  0.713096  0.769848  0.788410  0.780613  \n",
       "29  0.669863  0.715910  0.739774  0.840375  0.809299  0.768639  \n",
       "30  0.689799  0.783339  0.598768  0.625851  0.623240  0.722793  \n",
       "31  0.834170  0.722425  0.615877  0.622989  0.567113  0.688224  \n",
       "32  0.785248  0.849898  0.539665  0.640618  0.728240  0.822813  \n",
       "33  0.852035  0.908592  0.646222  0.737953  0.734187  0.818343  \n",
       "34  0.844725  0.859050  0.635763  0.675059  0.692186  0.793873  \n",
       "35  0.540646  0.698375  0.509632  0.549013  0.565088  0.624987  \n",
       "36  0.768282  0.786941  0.502812  0.728353  0.628853  0.654319  \n",
       "37  0.776409  0.685610  0.410371  0.611967  0.541319  0.559989  \n",
       "38  0.604297  0.763830  0.599706  0.562963  0.663477  0.706858  \n",
       "39  0.635976  0.674283  0.683660  0.596926  0.604378  0.625303  \n",
       "40  0.810940  0.890905  0.608256  0.671767  0.714214  0.789244  \n",
       "41  0.794073  0.771416  0.495346  0.652303  0.568583  0.642873  \n",
       "42  0.738202  0.847350  0.704555  0.733981  0.741210  0.751133  \n",
       "43  0.798355  0.752779  0.663390  0.795322  0.687058  0.718935  \n",
       "44  0.854777  0.793306  0.618322  0.750792  0.703394  0.799112  \n",
       "45  0.457360  0.651132  0.429106  0.427948  0.497080  0.541481  \n",
       "46  0.740476  0.862722  0.499067  0.655145  0.669871  0.713600  \n",
       "47  0.670162  0.721488  0.545915  0.517871  0.557607  0.633766  \n",
       "48  0.575359  0.731213  0.642383  0.620108  0.665620  0.715041  \n",
       "49  0.512744  0.657830  0.538586  0.490798  0.572200  0.603538  \n",
       "50  0.678939  0.560498  0.386559  0.380307  0.442160  0.541360  \n",
       "51  0.542782  0.519049  0.344661  0.334831  0.504676  0.582738  \n",
       "52  0.824888  0.818197  0.574108  0.708325  0.667350  0.699529  \n",
       "53  0.498010  0.624687  0.554217  0.499876  0.660626  0.713950  \n",
       "54  1.000000  0.814918  0.565864  0.692537  0.664510  0.742056  \n",
       "55  0.814918  1.000000  0.655187  0.790419  0.787582  0.843646  \n",
       "56  0.565864  0.655187  1.000000  0.802225  0.798727  0.745591  \n",
       "57  0.692537  0.790419  0.802225  1.000000  0.829152  0.822606  \n",
       "58  0.664510  0.787582  0.798727  0.829152  1.000000  0.880917  \n",
       "59  0.742056  0.843646  0.745591  0.822606  0.880917  1.000000  \n",
       "\n",
       "[60 rows x 61 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "neural_sim_df_reordered"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "548adf9f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>term</th>\n",
       "      <th>horse</th>\n",
       "      <th>bear</th>\n",
       "      <th>cat</th>\n",
       "      <th>dog</th>\n",
       "      <th>cow</th>\n",
       "      <th>leg</th>\n",
       "      <th>arm</th>\n",
       "      <th>hand</th>\n",
       "      <th>foot</th>\n",
       "      <th>...</th>\n",
       "      <th>tomato</th>\n",
       "      <th>celery</th>\n",
       "      <th>corn</th>\n",
       "      <th>carrot</th>\n",
       "      <th>lettuce</th>\n",
       "      <th>bicycle</th>\n",
       "      <th>car</th>\n",
       "      <th>train</th>\n",
       "      <th>truck</th>\n",
       "      <th>airplane</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>horse</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.867668</td>\n",
       "      <td>0.794402</td>\n",
       "      <td>0.817715</td>\n",
       "      <td>0.762252</td>\n",
       "      <td>0.655147</td>\n",
       "      <td>0.560490</td>\n",
       "      <td>0.798085</td>\n",
       "      <td>0.773818</td>\n",
       "      <td>...</td>\n",
       "      <td>0.679341</td>\n",
       "      <td>0.517919</td>\n",
       "      <td>0.809874</td>\n",
       "      <td>0.586457</td>\n",
       "      <td>0.814620</td>\n",
       "      <td>0.886597</td>\n",
       "      <td>0.591752</td>\n",
       "      <td>0.698553</td>\n",
       "      <td>0.770612</td>\n",
       "      <td>0.830323</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>bear</td>\n",
       "      <td>0.867668</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.784209</td>\n",
       "      <td>0.856424</td>\n",
       "      <td>0.842841</td>\n",
       "      <td>0.576497</td>\n",
       "      <td>0.529558</td>\n",
       "      <td>0.703956</td>\n",
       "      <td>0.695968</td>\n",
       "      <td>...</td>\n",
       "      <td>0.678013</td>\n",
       "      <td>0.514908</td>\n",
       "      <td>0.710625</td>\n",
       "      <td>0.626575</td>\n",
       "      <td>0.799161</td>\n",
       "      <td>0.849823</td>\n",
       "      <td>0.696179</td>\n",
       "      <td>0.748991</td>\n",
       "      <td>0.761983</td>\n",
       "      <td>0.828290</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>cat</td>\n",
       "      <td>0.794402</td>\n",
       "      <td>0.784209</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.801168</td>\n",
       "      <td>0.706735</td>\n",
       "      <td>0.798449</td>\n",
       "      <td>0.518258</td>\n",
       "      <td>0.803878</td>\n",
       "      <td>0.672096</td>\n",
       "      <td>...</td>\n",
       "      <td>0.571532</td>\n",
       "      <td>0.317849</td>\n",
       "      <td>0.771336</td>\n",
       "      <td>0.417673</td>\n",
       "      <td>0.768428</td>\n",
       "      <td>0.835674</td>\n",
       "      <td>0.513325</td>\n",
       "      <td>0.669761</td>\n",
       "      <td>0.608832</td>\n",
       "      <td>0.677270</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>dog</td>\n",
       "      <td>0.817715</td>\n",
       "      <td>0.856424</td>\n",
       "      <td>0.801168</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.869596</td>\n",
       "      <td>0.669203</td>\n",
       "      <td>0.587157</td>\n",
       "      <td>0.763389</td>\n",
       "      <td>0.744822</td>\n",
       "      <td>...</td>\n",
       "      <td>0.644053</td>\n",
       "      <td>0.475084</td>\n",
       "      <td>0.698746</td>\n",
       "      <td>0.537472</td>\n",
       "      <td>0.784360</td>\n",
       "      <td>0.831094</td>\n",
       "      <td>0.610807</td>\n",
       "      <td>0.618325</td>\n",
       "      <td>0.649231</td>\n",
       "      <td>0.724007</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>cow</td>\n",
       "      <td>0.762252</td>\n",
       "      <td>0.842841</td>\n",
       "      <td>0.706735</td>\n",
       "      <td>0.869596</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.564517</td>\n",
       "      <td>0.602313</td>\n",
       "      <td>0.636055</td>\n",
       "      <td>0.691362</td>\n",
       "      <td>...</td>\n",
       "      <td>0.561507</td>\n",
       "      <td>0.544593</td>\n",
       "      <td>0.655788</td>\n",
       "      <td>0.623611</td>\n",
       "      <td>0.740062</td>\n",
       "      <td>0.792543</td>\n",
       "      <td>0.700272</td>\n",
       "      <td>0.650916</td>\n",
       "      <td>0.690289</td>\n",
       "      <td>0.769298</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>leg</td>\n",
       "      <td>0.655147</td>\n",
       "      <td>0.576497</td>\n",
       "      <td>0.798449</td>\n",
       "      <td>0.669203</td>\n",
       "      <td>0.564517</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.589304</td>\n",
       "      <td>0.745915</td>\n",
       "      <td>0.706871</td>\n",
       "      <td>...</td>\n",
       "      <td>0.324236</td>\n",
       "      <td>0.241527</td>\n",
       "      <td>0.740154</td>\n",
       "      <td>0.298281</td>\n",
       "      <td>0.635136</td>\n",
       "      <td>0.779629</td>\n",
       "      <td>0.405805</td>\n",
       "      <td>0.586700</td>\n",
       "      <td>0.483917</td>\n",
       "      <td>0.521596</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>arm</td>\n",
       "      <td>0.560490</td>\n",
       "      <td>0.529558</td>\n",
       "      <td>0.518258</td>\n",
       "      <td>0.587157</td>\n",
       "      <td>0.602313</td>\n",
       "      <td>0.589304</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.683080</td>\n",
       "      <td>0.846284</td>\n",
       "      <td>...</td>\n",
       "      <td>0.211426</td>\n",
       "      <td>0.550446</td>\n",
       "      <td>0.497993</td>\n",
       "      <td>0.652083</td>\n",
       "      <td>0.436008</td>\n",
       "      <td>0.698630</td>\n",
       "      <td>0.389628</td>\n",
       "      <td>0.465599</td>\n",
       "      <td>0.566975</td>\n",
       "      <td>0.611928</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>hand</td>\n",
       "      <td>0.798085</td>\n",
       "      <td>0.703956</td>\n",
       "      <td>0.803878</td>\n",
       "      <td>0.763389</td>\n",
       "      <td>0.636055</td>\n",
       "      <td>0.745915</td>\n",
       "      <td>0.683080</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.840091</td>\n",
       "      <td>...</td>\n",
       "      <td>0.528932</td>\n",
       "      <td>0.426071</td>\n",
       "      <td>0.744326</td>\n",
       "      <td>0.466841</td>\n",
       "      <td>0.739569</td>\n",
       "      <td>0.838479</td>\n",
       "      <td>0.416598</td>\n",
       "      <td>0.552607</td>\n",
       "      <td>0.609400</td>\n",
       "      <td>0.669398</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>foot</td>\n",
       "      <td>0.773818</td>\n",
       "      <td>0.695968</td>\n",
       "      <td>0.672096</td>\n",
       "      <td>0.744822</td>\n",
       "      <td>0.691362</td>\n",
       "      <td>0.706871</td>\n",
       "      <td>0.846284</td>\n",
       "      <td>0.840091</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.449388</td>\n",
       "      <td>0.578857</td>\n",
       "      <td>0.695290</td>\n",
       "      <td>0.649962</td>\n",
       "      <td>0.644539</td>\n",
       "      <td>0.824975</td>\n",
       "      <td>0.479248</td>\n",
       "      <td>0.553614</td>\n",
       "      <td>0.694302</td>\n",
       "      <td>0.709507</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>eye</td>\n",
       "      <td>0.811312</td>\n",
       "      <td>0.797959</td>\n",
       "      <td>0.739001</td>\n",
       "      <td>0.761645</td>\n",
       "      <td>0.764978</td>\n",
       "      <td>0.678523</td>\n",
       "      <td>0.614393</td>\n",
       "      <td>0.756584</td>\n",
       "      <td>0.762380</td>\n",
       "      <td>...</td>\n",
       "      <td>0.518579</td>\n",
       "      <td>0.535730</td>\n",
       "      <td>0.842577</td>\n",
       "      <td>0.609926</td>\n",
       "      <td>0.822865</td>\n",
       "      <td>0.888571</td>\n",
       "      <td>0.742480</td>\n",
       "      <td>0.823249</td>\n",
       "      <td>0.822879</td>\n",
       "      <td>0.826608</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>apartment</td>\n",
       "      <td>0.698721</td>\n",
       "      <td>0.762514</td>\n",
       "      <td>0.596658</td>\n",
       "      <td>0.621620</td>\n",
       "      <td>0.608525</td>\n",
       "      <td>0.445907</td>\n",
       "      <td>0.231247</td>\n",
       "      <td>0.482260</td>\n",
       "      <td>0.448296</td>\n",
       "      <td>...</td>\n",
       "      <td>0.587824</td>\n",
       "      <td>0.295193</td>\n",
       "      <td>0.649879</td>\n",
       "      <td>0.399071</td>\n",
       "      <td>0.730971</td>\n",
       "      <td>0.689024</td>\n",
       "      <td>0.722454</td>\n",
       "      <td>0.822750</td>\n",
       "      <td>0.710393</td>\n",
       "      <td>0.733315</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>igloo</td>\n",
       "      <td>0.661859</td>\n",
       "      <td>0.688277</td>\n",
       "      <td>0.670116</td>\n",
       "      <td>0.640349</td>\n",
       "      <td>0.679977</td>\n",
       "      <td>0.570472</td>\n",
       "      <td>0.403760</td>\n",
       "      <td>0.536716</td>\n",
       "      <td>0.530904</td>\n",
       "      <td>...</td>\n",
       "      <td>0.415552</td>\n",
       "      <td>0.403774</td>\n",
       "      <td>0.759893</td>\n",
       "      <td>0.475588</td>\n",
       "      <td>0.739594</td>\n",
       "      <td>0.736073</td>\n",
       "      <td>0.753110</td>\n",
       "      <td>0.824186</td>\n",
       "      <td>0.746618</td>\n",
       "      <td>0.724672</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>barn</td>\n",
       "      <td>0.711925</td>\n",
       "      <td>0.737495</td>\n",
       "      <td>0.622361</td>\n",
       "      <td>0.619353</td>\n",
       "      <td>0.651241</td>\n",
       "      <td>0.470127</td>\n",
       "      <td>0.356480</td>\n",
       "      <td>0.513918</td>\n",
       "      <td>0.526572</td>\n",
       "      <td>...</td>\n",
       "      <td>0.560346</td>\n",
       "      <td>0.375587</td>\n",
       "      <td>0.661230</td>\n",
       "      <td>0.501225</td>\n",
       "      <td>0.706021</td>\n",
       "      <td>0.714799</td>\n",
       "      <td>0.758685</td>\n",
       "      <td>0.815631</td>\n",
       "      <td>0.817320</td>\n",
       "      <td>0.801912</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>house</td>\n",
       "      <td>0.635954</td>\n",
       "      <td>0.703997</td>\n",
       "      <td>0.529147</td>\n",
       "      <td>0.588550</td>\n",
       "      <td>0.651021</td>\n",
       "      <td>0.356240</td>\n",
       "      <td>0.172805</td>\n",
       "      <td>0.388721</td>\n",
       "      <td>0.360437</td>\n",
       "      <td>...</td>\n",
       "      <td>0.512642</td>\n",
       "      <td>0.301658</td>\n",
       "      <td>0.615431</td>\n",
       "      <td>0.407944</td>\n",
       "      <td>0.699130</td>\n",
       "      <td>0.633268</td>\n",
       "      <td>0.788316</td>\n",
       "      <td>0.815476</td>\n",
       "      <td>0.734614</td>\n",
       "      <td>0.730749</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>church</td>\n",
       "      <td>0.632664</td>\n",
       "      <td>0.686622</td>\n",
       "      <td>0.568706</td>\n",
       "      <td>0.601965</td>\n",
       "      <td>0.621611</td>\n",
       "      <td>0.481075</td>\n",
       "      <td>0.309655</td>\n",
       "      <td>0.449913</td>\n",
       "      <td>0.464908</td>\n",
       "      <td>...</td>\n",
       "      <td>0.460718</td>\n",
       "      <td>0.367576</td>\n",
       "      <td>0.673649</td>\n",
       "      <td>0.421551</td>\n",
       "      <td>0.727649</td>\n",
       "      <td>0.695612</td>\n",
       "      <td>0.714085</td>\n",
       "      <td>0.826501</td>\n",
       "      <td>0.705087</td>\n",
       "      <td>0.706253</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>door</td>\n",
       "      <td>0.664959</td>\n",
       "      <td>0.697948</td>\n",
       "      <td>0.707445</td>\n",
       "      <td>0.654743</td>\n",
       "      <td>0.614215</td>\n",
       "      <td>0.660156</td>\n",
       "      <td>0.389011</td>\n",
       "      <td>0.605931</td>\n",
       "      <td>0.573216</td>\n",
       "      <td>...</td>\n",
       "      <td>0.484830</td>\n",
       "      <td>0.277677</td>\n",
       "      <td>0.765617</td>\n",
       "      <td>0.370305</td>\n",
       "      <td>0.743025</td>\n",
       "      <td>0.744535</td>\n",
       "      <td>0.694549</td>\n",
       "      <td>0.837039</td>\n",
       "      <td>0.686668</td>\n",
       "      <td>0.699127</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>closet</td>\n",
       "      <td>0.715603</td>\n",
       "      <td>0.687266</td>\n",
       "      <td>0.673670</td>\n",
       "      <td>0.612249</td>\n",
       "      <td>0.605650</td>\n",
       "      <td>0.643443</td>\n",
       "      <td>0.464889</td>\n",
       "      <td>0.615073</td>\n",
       "      <td>0.621563</td>\n",
       "      <td>...</td>\n",
       "      <td>0.392241</td>\n",
       "      <td>0.312676</td>\n",
       "      <td>0.751869</td>\n",
       "      <td>0.463871</td>\n",
       "      <td>0.706825</td>\n",
       "      <td>0.801271</td>\n",
       "      <td>0.740680</td>\n",
       "      <td>0.883244</td>\n",
       "      <td>0.787451</td>\n",
       "      <td>0.748007</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>chimney</td>\n",
       "      <td>0.758883</td>\n",
       "      <td>0.759291</td>\n",
       "      <td>0.688216</td>\n",
       "      <td>0.698518</td>\n",
       "      <td>0.717023</td>\n",
       "      <td>0.673674</td>\n",
       "      <td>0.513790</td>\n",
       "      <td>0.589217</td>\n",
       "      <td>0.669381</td>\n",
       "      <td>...</td>\n",
       "      <td>0.451119</td>\n",
       "      <td>0.457083</td>\n",
       "      <td>0.772736</td>\n",
       "      <td>0.514505</td>\n",
       "      <td>0.763782</td>\n",
       "      <td>0.831779</td>\n",
       "      <td>0.743953</td>\n",
       "      <td>0.859954</td>\n",
       "      <td>0.767059</td>\n",
       "      <td>0.789290</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>window</td>\n",
       "      <td>0.762504</td>\n",
       "      <td>0.718538</td>\n",
       "      <td>0.666203</td>\n",
       "      <td>0.622478</td>\n",
       "      <td>0.579022</td>\n",
       "      <td>0.610803</td>\n",
       "      <td>0.376803</td>\n",
       "      <td>0.623167</td>\n",
       "      <td>0.589509</td>\n",
       "      <td>...</td>\n",
       "      <td>0.586896</td>\n",
       "      <td>0.428371</td>\n",
       "      <td>0.784558</td>\n",
       "      <td>0.458079</td>\n",
       "      <td>0.786179</td>\n",
       "      <td>0.769819</td>\n",
       "      <td>0.670236</td>\n",
       "      <td>0.804625</td>\n",
       "      <td>0.735202</td>\n",
       "      <td>0.742122</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>arch</td>\n",
       "      <td>0.610093</td>\n",
       "      <td>0.671840</td>\n",
       "      <td>0.491565</td>\n",
       "      <td>0.505957</td>\n",
       "      <td>0.506032</td>\n",
       "      <td>0.395335</td>\n",
       "      <td>0.280516</td>\n",
       "      <td>0.423684</td>\n",
       "      <td>0.435040</td>\n",
       "      <td>...</td>\n",
       "      <td>0.525820</td>\n",
       "      <td>0.316577</td>\n",
       "      <td>0.620526</td>\n",
       "      <td>0.438808</td>\n",
       "      <td>0.636221</td>\n",
       "      <td>0.630249</td>\n",
       "      <td>0.692751</td>\n",
       "      <td>0.800162</td>\n",
       "      <td>0.693101</td>\n",
       "      <td>0.709135</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>shirt</td>\n",
       "      <td>0.878576</td>\n",
       "      <td>0.834256</td>\n",
       "      <td>0.813089</td>\n",
       "      <td>0.840423</td>\n",
       "      <td>0.777312</td>\n",
       "      <td>0.678780</td>\n",
       "      <td>0.494433</td>\n",
       "      <td>0.778000</td>\n",
       "      <td>0.727838</td>\n",
       "      <td>...</td>\n",
       "      <td>0.741920</td>\n",
       "      <td>0.529024</td>\n",
       "      <td>0.819480</td>\n",
       "      <td>0.538137</td>\n",
       "      <td>0.886150</td>\n",
       "      <td>0.848812</td>\n",
       "      <td>0.582945</td>\n",
       "      <td>0.659794</td>\n",
       "      <td>0.675980</td>\n",
       "      <td>0.766515</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>pants</td>\n",
       "      <td>0.777598</td>\n",
       "      <td>0.736485</td>\n",
       "      <td>0.794516</td>\n",
       "      <td>0.776590</td>\n",
       "      <td>0.731372</td>\n",
       "      <td>0.805345</td>\n",
       "      <td>0.596718</td>\n",
       "      <td>0.750308</td>\n",
       "      <td>0.754078</td>\n",
       "      <td>...</td>\n",
       "      <td>0.492070</td>\n",
       "      <td>0.477946</td>\n",
       "      <td>0.858527</td>\n",
       "      <td>0.495714</td>\n",
       "      <td>0.809120</td>\n",
       "      <td>0.848815</td>\n",
       "      <td>0.570421</td>\n",
       "      <td>0.724547</td>\n",
       "      <td>0.629747</td>\n",
       "      <td>0.704129</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>skirt</td>\n",
       "      <td>0.822725</td>\n",
       "      <td>0.790998</td>\n",
       "      <td>0.769078</td>\n",
       "      <td>0.799265</td>\n",
       "      <td>0.749963</td>\n",
       "      <td>0.688187</td>\n",
       "      <td>0.486777</td>\n",
       "      <td>0.759455</td>\n",
       "      <td>0.678832</td>\n",
       "      <td>...</td>\n",
       "      <td>0.587417</td>\n",
       "      <td>0.492523</td>\n",
       "      <td>0.792462</td>\n",
       "      <td>0.491346</td>\n",
       "      <td>0.854875</td>\n",
       "      <td>0.833014</td>\n",
       "      <td>0.564682</td>\n",
       "      <td>0.670851</td>\n",
       "      <td>0.626155</td>\n",
       "      <td>0.729970</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>coat</td>\n",
       "      <td>0.889299</td>\n",
       "      <td>0.816334</td>\n",
       "      <td>0.796006</td>\n",
       "      <td>0.839576</td>\n",
       "      <td>0.808395</td>\n",
       "      <td>0.746119</td>\n",
       "      <td>0.645293</td>\n",
       "      <td>0.794849</td>\n",
       "      <td>0.811017</td>\n",
       "      <td>...</td>\n",
       "      <td>0.641827</td>\n",
       "      <td>0.586026</td>\n",
       "      <td>0.854764</td>\n",
       "      <td>0.623968</td>\n",
       "      <td>0.839451</td>\n",
       "      <td>0.893446</td>\n",
       "      <td>0.635014</td>\n",
       "      <td>0.717671</td>\n",
       "      <td>0.736458</td>\n",
       "      <td>0.817678</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>dress</td>\n",
       "      <td>0.830805</td>\n",
       "      <td>0.784709</td>\n",
       "      <td>0.832871</td>\n",
       "      <td>0.818860</td>\n",
       "      <td>0.754664</td>\n",
       "      <td>0.778052</td>\n",
       "      <td>0.627562</td>\n",
       "      <td>0.820303</td>\n",
       "      <td>0.787593</td>\n",
       "      <td>...</td>\n",
       "      <td>0.583753</td>\n",
       "      <td>0.506032</td>\n",
       "      <td>0.829335</td>\n",
       "      <td>0.539895</td>\n",
       "      <td>0.828975</td>\n",
       "      <td>0.870090</td>\n",
       "      <td>0.535723</td>\n",
       "      <td>0.691407</td>\n",
       "      <td>0.642177</td>\n",
       "      <td>0.749929</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>chair</td>\n",
       "      <td>0.768306</td>\n",
       "      <td>0.672516</td>\n",
       "      <td>0.747876</td>\n",
       "      <td>0.667374</td>\n",
       "      <td>0.600179</td>\n",
       "      <td>0.726622</td>\n",
       "      <td>0.494652</td>\n",
       "      <td>0.716269</td>\n",
       "      <td>0.686106</td>\n",
       "      <td>...</td>\n",
       "      <td>0.506024</td>\n",
       "      <td>0.398247</td>\n",
       "      <td>0.847935</td>\n",
       "      <td>0.454849</td>\n",
       "      <td>0.789435</td>\n",
       "      <td>0.817605</td>\n",
       "      <td>0.583464</td>\n",
       "      <td>0.757182</td>\n",
       "      <td>0.698082</td>\n",
       "      <td>0.690255</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>dresser</td>\n",
       "      <td>0.788659</td>\n",
       "      <td>0.722362</td>\n",
       "      <td>0.595316</td>\n",
       "      <td>0.616838</td>\n",
       "      <td>0.617454</td>\n",
       "      <td>0.481023</td>\n",
       "      <td>0.452728</td>\n",
       "      <td>0.643746</td>\n",
       "      <td>0.639404</td>\n",
       "      <td>...</td>\n",
       "      <td>0.565132</td>\n",
       "      <td>0.487753</td>\n",
       "      <td>0.785302</td>\n",
       "      <td>0.534382</td>\n",
       "      <td>0.770415</td>\n",
       "      <td>0.757685</td>\n",
       "      <td>0.676241</td>\n",
       "      <td>0.781657</td>\n",
       "      <td>0.799060</td>\n",
       "      <td>0.787166</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>bed</td>\n",
       "      <td>0.725328</td>\n",
       "      <td>0.751571</td>\n",
       "      <td>0.598146</td>\n",
       "      <td>0.701655</td>\n",
       "      <td>0.736045</td>\n",
       "      <td>0.547282</td>\n",
       "      <td>0.465465</td>\n",
       "      <td>0.535473</td>\n",
       "      <td>0.619548</td>\n",
       "      <td>...</td>\n",
       "      <td>0.475872</td>\n",
       "      <td>0.483395</td>\n",
       "      <td>0.695193</td>\n",
       "      <td>0.554463</td>\n",
       "      <td>0.728105</td>\n",
       "      <td>0.771604</td>\n",
       "      <td>0.761585</td>\n",
       "      <td>0.770276</td>\n",
       "      <td>0.746601</td>\n",
       "      <td>0.729726</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>table</td>\n",
       "      <td>0.726813</td>\n",
       "      <td>0.676546</td>\n",
       "      <td>0.579006</td>\n",
       "      <td>0.631826</td>\n",
       "      <td>0.659164</td>\n",
       "      <td>0.607550</td>\n",
       "      <td>0.646708</td>\n",
       "      <td>0.617186</td>\n",
       "      <td>0.754221</td>\n",
       "      <td>...</td>\n",
       "      <td>0.428788</td>\n",
       "      <td>0.543479</td>\n",
       "      <td>0.757842</td>\n",
       "      <td>0.608505</td>\n",
       "      <td>0.666310</td>\n",
       "      <td>0.784173</td>\n",
       "      <td>0.713096</td>\n",
       "      <td>0.769848</td>\n",
       "      <td>0.788410</td>\n",
       "      <td>0.780613</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>desk</td>\n",
       "      <td>0.684069</td>\n",
       "      <td>0.627647</td>\n",
       "      <td>0.501265</td>\n",
       "      <td>0.530369</td>\n",
       "      <td>0.567477</td>\n",
       "      <td>0.484901</td>\n",
       "      <td>0.432561</td>\n",
       "      <td>0.496075</td>\n",
       "      <td>0.557414</td>\n",
       "      <td>...</td>\n",
       "      <td>0.384405</td>\n",
       "      <td>0.404495</td>\n",
       "      <td>0.699622</td>\n",
       "      <td>0.484119</td>\n",
       "      <td>0.669863</td>\n",
       "      <td>0.715910</td>\n",
       "      <td>0.739774</td>\n",
       "      <td>0.840375</td>\n",
       "      <td>0.809299</td>\n",
       "      <td>0.768639</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>ant</td>\n",
       "      <td>0.691065</td>\n",
       "      <td>0.777583</td>\n",
       "      <td>0.754469</td>\n",
       "      <td>0.800461</td>\n",
       "      <td>0.834809</td>\n",
       "      <td>0.636324</td>\n",
       "      <td>0.635829</td>\n",
       "      <td>0.691927</td>\n",
       "      <td>0.659346</td>\n",
       "      <td>...</td>\n",
       "      <td>0.411848</td>\n",
       "      <td>0.512513</td>\n",
       "      <td>0.619320</td>\n",
       "      <td>0.554288</td>\n",
       "      <td>0.689799</td>\n",
       "      <td>0.783339</td>\n",
       "      <td>0.598768</td>\n",
       "      <td>0.625851</td>\n",
       "      <td>0.623240</td>\n",
       "      <td>0.722793</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>butterfly</td>\n",
       "      <td>0.738073</td>\n",
       "      <td>0.782226</td>\n",
       "      <td>0.704329</td>\n",
       "      <td>0.776244</td>\n",
       "      <td>0.729092</td>\n",
       "      <td>0.501622</td>\n",
       "      <td>0.300020</td>\n",
       "      <td>0.634290</td>\n",
       "      <td>0.525791</td>\n",
       "      <td>...</td>\n",
       "      <td>0.728920</td>\n",
       "      <td>0.401224</td>\n",
       "      <td>0.703990</td>\n",
       "      <td>0.470192</td>\n",
       "      <td>0.834170</td>\n",
       "      <td>0.722425</td>\n",
       "      <td>0.615877</td>\n",
       "      <td>0.622989</td>\n",
       "      <td>0.567113</td>\n",
       "      <td>0.688224</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>fly</td>\n",
       "      <td>0.870237</td>\n",
       "      <td>0.852732</td>\n",
       "      <td>0.799885</td>\n",
       "      <td>0.828591</td>\n",
       "      <td>0.807930</td>\n",
       "      <td>0.631929</td>\n",
       "      <td>0.632948</td>\n",
       "      <td>0.802939</td>\n",
       "      <td>0.762990</td>\n",
       "      <td>...</td>\n",
       "      <td>0.607168</td>\n",
       "      <td>0.572116</td>\n",
       "      <td>0.743052</td>\n",
       "      <td>0.604350</td>\n",
       "      <td>0.785248</td>\n",
       "      <td>0.849898</td>\n",
       "      <td>0.539665</td>\n",
       "      <td>0.640618</td>\n",
       "      <td>0.728240</td>\n",
       "      <td>0.822813</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>bee</td>\n",
       "      <td>0.902443</td>\n",
       "      <td>0.890533</td>\n",
       "      <td>0.875222</td>\n",
       "      <td>0.878088</td>\n",
       "      <td>0.826075</td>\n",
       "      <td>0.736508</td>\n",
       "      <td>0.567510</td>\n",
       "      <td>0.819981</td>\n",
       "      <td>0.758506</td>\n",
       "      <td>...</td>\n",
       "      <td>0.677455</td>\n",
       "      <td>0.506895</td>\n",
       "      <td>0.844244</td>\n",
       "      <td>0.581631</td>\n",
       "      <td>0.852035</td>\n",
       "      <td>0.908592</td>\n",
       "      <td>0.646222</td>\n",
       "      <td>0.737953</td>\n",
       "      <td>0.734187</td>\n",
       "      <td>0.818343</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>beetle</td>\n",
       "      <td>0.857445</td>\n",
       "      <td>0.866394</td>\n",
       "      <td>0.785367</td>\n",
       "      <td>0.859938</td>\n",
       "      <td>0.809364</td>\n",
       "      <td>0.643236</td>\n",
       "      <td>0.531996</td>\n",
       "      <td>0.761997</td>\n",
       "      <td>0.725093</td>\n",
       "      <td>...</td>\n",
       "      <td>0.669382</td>\n",
       "      <td>0.562920</td>\n",
       "      <td>0.776933</td>\n",
       "      <td>0.635357</td>\n",
       "      <td>0.844725</td>\n",
       "      <td>0.859050</td>\n",
       "      <td>0.635763</td>\n",
       "      <td>0.675059</td>\n",
       "      <td>0.692186</td>\n",
       "      <td>0.793873</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>knife</td>\n",
       "      <td>0.587953</td>\n",
       "      <td>0.616830</td>\n",
       "      <td>0.514735</td>\n",
       "      <td>0.576219</td>\n",
       "      <td>0.615567</td>\n",
       "      <td>0.544385</td>\n",
       "      <td>0.702187</td>\n",
       "      <td>0.568099</td>\n",
       "      <td>0.705813</td>\n",
       "      <td>...</td>\n",
       "      <td>0.413247</td>\n",
       "      <td>0.654923</td>\n",
       "      <td>0.611017</td>\n",
       "      <td>0.753891</td>\n",
       "      <td>0.540646</td>\n",
       "      <td>0.698375</td>\n",
       "      <td>0.509632</td>\n",
       "      <td>0.549013</td>\n",
       "      <td>0.565088</td>\n",
       "      <td>0.624987</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>bottle</td>\n",
       "      <td>0.695385</td>\n",
       "      <td>0.644273</td>\n",
       "      <td>0.758616</td>\n",
       "      <td>0.664466</td>\n",
       "      <td>0.583204</td>\n",
       "      <td>0.768683</td>\n",
       "      <td>0.539871</td>\n",
       "      <td>0.715812</td>\n",
       "      <td>0.694144</td>\n",
       "      <td>...</td>\n",
       "      <td>0.493462</td>\n",
       "      <td>0.429526</td>\n",
       "      <td>0.832812</td>\n",
       "      <td>0.440403</td>\n",
       "      <td>0.768282</td>\n",
       "      <td>0.786941</td>\n",
       "      <td>0.502812</td>\n",
       "      <td>0.728353</td>\n",
       "      <td>0.628853</td>\n",
       "      <td>0.654319</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>glass</td>\n",
       "      <td>0.669068</td>\n",
       "      <td>0.609779</td>\n",
       "      <td>0.716930</td>\n",
       "      <td>0.602703</td>\n",
       "      <td>0.465573</td>\n",
       "      <td>0.628759</td>\n",
       "      <td>0.303552</td>\n",
       "      <td>0.672058</td>\n",
       "      <td>0.564443</td>\n",
       "      <td>...</td>\n",
       "      <td>0.618897</td>\n",
       "      <td>0.352579</td>\n",
       "      <td>0.770443</td>\n",
       "      <td>0.323041</td>\n",
       "      <td>0.776409</td>\n",
       "      <td>0.685610</td>\n",
       "      <td>0.410371</td>\n",
       "      <td>0.611967</td>\n",
       "      <td>0.541319</td>\n",
       "      <td>0.559989</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>spoon</td>\n",
       "      <td>0.689416</td>\n",
       "      <td>0.703103</td>\n",
       "      <td>0.557828</td>\n",
       "      <td>0.682571</td>\n",
       "      <td>0.697471</td>\n",
       "      <td>0.555059</td>\n",
       "      <td>0.722344</td>\n",
       "      <td>0.649860</td>\n",
       "      <td>0.789202</td>\n",
       "      <td>...</td>\n",
       "      <td>0.513288</td>\n",
       "      <td>0.699333</td>\n",
       "      <td>0.613033</td>\n",
       "      <td>0.820611</td>\n",
       "      <td>0.604297</td>\n",
       "      <td>0.763830</td>\n",
       "      <td>0.599706</td>\n",
       "      <td>0.562963</td>\n",
       "      <td>0.663477</td>\n",
       "      <td>0.706858</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>cup</td>\n",
       "      <td>0.631058</td>\n",
       "      <td>0.672261</td>\n",
       "      <td>0.557587</td>\n",
       "      <td>0.678326</td>\n",
       "      <td>0.642021</td>\n",
       "      <td>0.523357</td>\n",
       "      <td>0.439673</td>\n",
       "      <td>0.570131</td>\n",
       "      <td>0.596660</td>\n",
       "      <td>...</td>\n",
       "      <td>0.680988</td>\n",
       "      <td>0.474785</td>\n",
       "      <td>0.656271</td>\n",
       "      <td>0.595870</td>\n",
       "      <td>0.635976</td>\n",
       "      <td>0.674283</td>\n",
       "      <td>0.683660</td>\n",
       "      <td>0.596926</td>\n",
       "      <td>0.604378</td>\n",
       "      <td>0.625303</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>telephone</td>\n",
       "      <td>0.907903</td>\n",
       "      <td>0.809121</td>\n",
       "      <td>0.770225</td>\n",
       "      <td>0.807333</td>\n",
       "      <td>0.764701</td>\n",
       "      <td>0.724612</td>\n",
       "      <td>0.574842</td>\n",
       "      <td>0.772525</td>\n",
       "      <td>0.773986</td>\n",
       "      <td>...</td>\n",
       "      <td>0.645410</td>\n",
       "      <td>0.546342</td>\n",
       "      <td>0.818373</td>\n",
       "      <td>0.585782</td>\n",
       "      <td>0.810940</td>\n",
       "      <td>0.890905</td>\n",
       "      <td>0.608256</td>\n",
       "      <td>0.671767</td>\n",
       "      <td>0.714214</td>\n",
       "      <td>0.789244</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>key</td>\n",
       "      <td>0.697340</td>\n",
       "      <td>0.703172</td>\n",
       "      <td>0.850677</td>\n",
       "      <td>0.734032</td>\n",
       "      <td>0.635849</td>\n",
       "      <td>0.761813</td>\n",
       "      <td>0.451965</td>\n",
       "      <td>0.772869</td>\n",
       "      <td>0.624108</td>\n",
       "      <td>...</td>\n",
       "      <td>0.563528</td>\n",
       "      <td>0.350159</td>\n",
       "      <td>0.824987</td>\n",
       "      <td>0.384281</td>\n",
       "      <td>0.794073</td>\n",
       "      <td>0.771416</td>\n",
       "      <td>0.495346</td>\n",
       "      <td>0.652303</td>\n",
       "      <td>0.568583</td>\n",
       "      <td>0.642873</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>watch</td>\n",
       "      <td>0.756502</td>\n",
       "      <td>0.698777</td>\n",
       "      <td>0.678881</td>\n",
       "      <td>0.727094</td>\n",
       "      <td>0.746676</td>\n",
       "      <td>0.739074</td>\n",
       "      <td>0.644681</td>\n",
       "      <td>0.672264</td>\n",
       "      <td>0.781115</td>\n",
       "      <td>...</td>\n",
       "      <td>0.424524</td>\n",
       "      <td>0.554331</td>\n",
       "      <td>0.811393</td>\n",
       "      <td>0.617313</td>\n",
       "      <td>0.738202</td>\n",
       "      <td>0.847350</td>\n",
       "      <td>0.704555</td>\n",
       "      <td>0.733981</td>\n",
       "      <td>0.741210</td>\n",
       "      <td>0.751133</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>refrigerator</td>\n",
       "      <td>0.745210</td>\n",
       "      <td>0.726670</td>\n",
       "      <td>0.698859</td>\n",
       "      <td>0.658273</td>\n",
       "      <td>0.641891</td>\n",
       "      <td>0.618369</td>\n",
       "      <td>0.334180</td>\n",
       "      <td>0.556853</td>\n",
       "      <td>0.545528</td>\n",
       "      <td>...</td>\n",
       "      <td>0.590699</td>\n",
       "      <td>0.427098</td>\n",
       "      <td>0.799737</td>\n",
       "      <td>0.481697</td>\n",
       "      <td>0.798355</td>\n",
       "      <td>0.752779</td>\n",
       "      <td>0.663390</td>\n",
       "      <td>0.795322</td>\n",
       "      <td>0.687058</td>\n",
       "      <td>0.718935</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>bell</td>\n",
       "      <td>0.761587</td>\n",
       "      <td>0.770673</td>\n",
       "      <td>0.752112</td>\n",
       "      <td>0.704024</td>\n",
       "      <td>0.705145</td>\n",
       "      <td>0.596411</td>\n",
       "      <td>0.459060</td>\n",
       "      <td>0.664241</td>\n",
       "      <td>0.624405</td>\n",
       "      <td>...</td>\n",
       "      <td>0.633726</td>\n",
       "      <td>0.540109</td>\n",
       "      <td>0.809442</td>\n",
       "      <td>0.577536</td>\n",
       "      <td>0.854777</td>\n",
       "      <td>0.793306</td>\n",
       "      <td>0.618322</td>\n",
       "      <td>0.750792</td>\n",
       "      <td>0.703394</td>\n",
       "      <td>0.799112</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>screwdriver</td>\n",
       "      <td>0.525716</td>\n",
       "      <td>0.512598</td>\n",
       "      <td>0.416282</td>\n",
       "      <td>0.524439</td>\n",
       "      <td>0.564325</td>\n",
       "      <td>0.562187</td>\n",
       "      <td>0.744482</td>\n",
       "      <td>0.526077</td>\n",
       "      <td>0.747239</td>\n",
       "      <td>...</td>\n",
       "      <td>0.276495</td>\n",
       "      <td>0.665802</td>\n",
       "      <td>0.544270</td>\n",
       "      <td>0.737703</td>\n",
       "      <td>0.457360</td>\n",
       "      <td>0.651132</td>\n",
       "      <td>0.429106</td>\n",
       "      <td>0.427948</td>\n",
       "      <td>0.497080</td>\n",
       "      <td>0.541481</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>chisel</td>\n",
       "      <td>0.837904</td>\n",
       "      <td>0.714995</td>\n",
       "      <td>0.777330</td>\n",
       "      <td>0.718992</td>\n",
       "      <td>0.638073</td>\n",
       "      <td>0.789969</td>\n",
       "      <td>0.624913</td>\n",
       "      <td>0.791892</td>\n",
       "      <td>0.802794</td>\n",
       "      <td>...</td>\n",
       "      <td>0.569381</td>\n",
       "      <td>0.479680</td>\n",
       "      <td>0.851834</td>\n",
       "      <td>0.553871</td>\n",
       "      <td>0.740476</td>\n",
       "      <td>0.862722</td>\n",
       "      <td>0.499067</td>\n",
       "      <td>0.655145</td>\n",
       "      <td>0.669871</td>\n",
       "      <td>0.713600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>pliers</td>\n",
       "      <td>0.662072</td>\n",
       "      <td>0.683640</td>\n",
       "      <td>0.563572</td>\n",
       "      <td>0.703940</td>\n",
       "      <td>0.724202</td>\n",
       "      <td>0.587292</td>\n",
       "      <td>0.602502</td>\n",
       "      <td>0.586857</td>\n",
       "      <td>0.710188</td>\n",
       "      <td>...</td>\n",
       "      <td>0.526958</td>\n",
       "      <td>0.667040</td>\n",
       "      <td>0.687934</td>\n",
       "      <td>0.653829</td>\n",
       "      <td>0.670162</td>\n",
       "      <td>0.721488</td>\n",
       "      <td>0.545915</td>\n",
       "      <td>0.517871</td>\n",
       "      <td>0.557607</td>\n",
       "      <td>0.633766</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>saw</td>\n",
       "      <td>0.638146</td>\n",
       "      <td>0.679344</td>\n",
       "      <td>0.538485</td>\n",
       "      <td>0.648948</td>\n",
       "      <td>0.691530</td>\n",
       "      <td>0.543095</td>\n",
       "      <td>0.712552</td>\n",
       "      <td>0.576657</td>\n",
       "      <td>0.746569</td>\n",
       "      <td>...</td>\n",
       "      <td>0.490540</td>\n",
       "      <td>0.650495</td>\n",
       "      <td>0.628111</td>\n",
       "      <td>0.784718</td>\n",
       "      <td>0.575359</td>\n",
       "      <td>0.731213</td>\n",
       "      <td>0.642383</td>\n",
       "      <td>0.620108</td>\n",
       "      <td>0.665620</td>\n",
       "      <td>0.715041</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>hammer</td>\n",
       "      <td>0.556451</td>\n",
       "      <td>0.545172</td>\n",
       "      <td>0.433060</td>\n",
       "      <td>0.577171</td>\n",
       "      <td>0.636218</td>\n",
       "      <td>0.524448</td>\n",
       "      <td>0.717967</td>\n",
       "      <td>0.522666</td>\n",
       "      <td>0.717579</td>\n",
       "      <td>...</td>\n",
       "      <td>0.360475</td>\n",
       "      <td>0.708623</td>\n",
       "      <td>0.579661</td>\n",
       "      <td>0.769853</td>\n",
       "      <td>0.512744</td>\n",
       "      <td>0.657830</td>\n",
       "      <td>0.538586</td>\n",
       "      <td>0.490798</td>\n",
       "      <td>0.572200</td>\n",
       "      <td>0.603538</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50</th>\n",
       "      <td>tomato</td>\n",
       "      <td>0.679341</td>\n",
       "      <td>0.678013</td>\n",
       "      <td>0.571532</td>\n",
       "      <td>0.644053</td>\n",
       "      <td>0.561507</td>\n",
       "      <td>0.324236</td>\n",
       "      <td>0.211426</td>\n",
       "      <td>0.528932</td>\n",
       "      <td>0.449388</td>\n",
       "      <td>...</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.500313</td>\n",
       "      <td>0.561782</td>\n",
       "      <td>0.506339</td>\n",
       "      <td>0.678939</td>\n",
       "      <td>0.560498</td>\n",
       "      <td>0.386559</td>\n",
       "      <td>0.380307</td>\n",
       "      <td>0.442160</td>\n",
       "      <td>0.541360</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51</th>\n",
       "      <td>celery</td>\n",
       "      <td>0.517919</td>\n",
       "      <td>0.514908</td>\n",
       "      <td>0.317849</td>\n",
       "      <td>0.475084</td>\n",
       "      <td>0.544593</td>\n",
       "      <td>0.241527</td>\n",
       "      <td>0.550446</td>\n",
       "      <td>0.426071</td>\n",
       "      <td>0.578857</td>\n",
       "      <td>...</td>\n",
       "      <td>0.500313</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.475159</td>\n",
       "      <td>0.763209</td>\n",
       "      <td>0.542782</td>\n",
       "      <td>0.519049</td>\n",
       "      <td>0.344661</td>\n",
       "      <td>0.334831</td>\n",
       "      <td>0.504676</td>\n",
       "      <td>0.582738</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>52</th>\n",
       "      <td>corn</td>\n",
       "      <td>0.809874</td>\n",
       "      <td>0.710625</td>\n",
       "      <td>0.771336</td>\n",
       "      <td>0.698746</td>\n",
       "      <td>0.655788</td>\n",
       "      <td>0.740154</td>\n",
       "      <td>0.497993</td>\n",
       "      <td>0.744326</td>\n",
       "      <td>0.695290</td>\n",
       "      <td>...</td>\n",
       "      <td>0.561782</td>\n",
       "      <td>0.475159</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.456373</td>\n",
       "      <td>0.824888</td>\n",
       "      <td>0.818197</td>\n",
       "      <td>0.574108</td>\n",
       "      <td>0.708325</td>\n",
       "      <td>0.667350</td>\n",
       "      <td>0.699529</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53</th>\n",
       "      <td>carrot</td>\n",
       "      <td>0.586457</td>\n",
       "      <td>0.626575</td>\n",
       "      <td>0.417673</td>\n",
       "      <td>0.537472</td>\n",
       "      <td>0.623611</td>\n",
       "      <td>0.298281</td>\n",
       "      <td>0.652083</td>\n",
       "      <td>0.466841</td>\n",
       "      <td>0.649962</td>\n",
       "      <td>...</td>\n",
       "      <td>0.506339</td>\n",
       "      <td>0.763209</td>\n",
       "      <td>0.456373</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.498010</td>\n",
       "      <td>0.624687</td>\n",
       "      <td>0.554217</td>\n",
       "      <td>0.499876</td>\n",
       "      <td>0.660626</td>\n",
       "      <td>0.713950</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>54</th>\n",
       "      <td>lettuce</td>\n",
       "      <td>0.814620</td>\n",
       "      <td>0.799161</td>\n",
       "      <td>0.768428</td>\n",
       "      <td>0.784360</td>\n",
       "      <td>0.740062</td>\n",
       "      <td>0.635136</td>\n",
       "      <td>0.436008</td>\n",
       "      <td>0.739569</td>\n",
       "      <td>0.644539</td>\n",
       "      <td>...</td>\n",
       "      <td>0.678939</td>\n",
       "      <td>0.542782</td>\n",
       "      <td>0.824888</td>\n",
       "      <td>0.498010</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.814918</td>\n",
       "      <td>0.565864</td>\n",
       "      <td>0.692537</td>\n",
       "      <td>0.664510</td>\n",
       "      <td>0.742056</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>55</th>\n",
       "      <td>bicycle</td>\n",
       "      <td>0.886597</td>\n",
       "      <td>0.849823</td>\n",
       "      <td>0.835674</td>\n",
       "      <td>0.831094</td>\n",
       "      <td>0.792543</td>\n",
       "      <td>0.779629</td>\n",
       "      <td>0.698630</td>\n",
       "      <td>0.838479</td>\n",
       "      <td>0.824975</td>\n",
       "      <td>...</td>\n",
       "      <td>0.560498</td>\n",
       "      <td>0.519049</td>\n",
       "      <td>0.818197</td>\n",
       "      <td>0.624687</td>\n",
       "      <td>0.814918</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.655187</td>\n",
       "      <td>0.790419</td>\n",
       "      <td>0.787582</td>\n",
       "      <td>0.843646</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>56</th>\n",
       "      <td>car</td>\n",
       "      <td>0.591752</td>\n",
       "      <td>0.696179</td>\n",
       "      <td>0.513325</td>\n",
       "      <td>0.610807</td>\n",
       "      <td>0.700272</td>\n",
       "      <td>0.405805</td>\n",
       "      <td>0.389628</td>\n",
       "      <td>0.416598</td>\n",
       "      <td>0.479248</td>\n",
       "      <td>...</td>\n",
       "      <td>0.386559</td>\n",
       "      <td>0.344661</td>\n",
       "      <td>0.574108</td>\n",
       "      <td>0.554217</td>\n",
       "      <td>0.565864</td>\n",
       "      <td>0.655187</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.802225</td>\n",
       "      <td>0.798727</td>\n",
       "      <td>0.745591</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>57</th>\n",
       "      <td>train</td>\n",
       "      <td>0.698553</td>\n",
       "      <td>0.748991</td>\n",
       "      <td>0.669761</td>\n",
       "      <td>0.618325</td>\n",
       "      <td>0.650916</td>\n",
       "      <td>0.586700</td>\n",
       "      <td>0.465599</td>\n",
       "      <td>0.552607</td>\n",
       "      <td>0.553614</td>\n",
       "      <td>...</td>\n",
       "      <td>0.380307</td>\n",
       "      <td>0.334831</td>\n",
       "      <td>0.708325</td>\n",
       "      <td>0.499876</td>\n",
       "      <td>0.692537</td>\n",
       "      <td>0.790419</td>\n",
       "      <td>0.802225</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.829152</td>\n",
       "      <td>0.822606</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>58</th>\n",
       "      <td>truck</td>\n",
       "      <td>0.770612</td>\n",
       "      <td>0.761983</td>\n",
       "      <td>0.608832</td>\n",
       "      <td>0.649231</td>\n",
       "      <td>0.690289</td>\n",
       "      <td>0.483917</td>\n",
       "      <td>0.566975</td>\n",
       "      <td>0.609400</td>\n",
       "      <td>0.694302</td>\n",
       "      <td>...</td>\n",
       "      <td>0.442160</td>\n",
       "      <td>0.504676</td>\n",
       "      <td>0.667350</td>\n",
       "      <td>0.660626</td>\n",
       "      <td>0.664510</td>\n",
       "      <td>0.787582</td>\n",
       "      <td>0.798727</td>\n",
       "      <td>0.829152</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.880917</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59</th>\n",
       "      <td>airplane</td>\n",
       "      <td>0.830323</td>\n",
       "      <td>0.828290</td>\n",
       "      <td>0.677270</td>\n",
       "      <td>0.724007</td>\n",
       "      <td>0.769298</td>\n",
       "      <td>0.521596</td>\n",
       "      <td>0.611928</td>\n",
       "      <td>0.669398</td>\n",
       "      <td>0.709507</td>\n",
       "      <td>...</td>\n",
       "      <td>0.541360</td>\n",
       "      <td>0.582738</td>\n",
       "      <td>0.699529</td>\n",
       "      <td>0.713950</td>\n",
       "      <td>0.742056</td>\n",
       "      <td>0.843646</td>\n",
       "      <td>0.745591</td>\n",
       "      <td>0.822606</td>\n",
       "      <td>0.880917</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>60 rows × 61 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            term     horse      bear       cat       dog       cow       leg  \\\n",
       "0          horse  1.000000  0.867668  0.794402  0.817715  0.762252  0.655147   \n",
       "1           bear  0.867668  1.000000  0.784209  0.856424  0.842841  0.576497   \n",
       "2            cat  0.794402  0.784209  1.000000  0.801168  0.706735  0.798449   \n",
       "3            dog  0.817715  0.856424  0.801168  1.000000  0.869596  0.669203   \n",
       "4            cow  0.762252  0.842841  0.706735  0.869596  1.000000  0.564517   \n",
       "5            leg  0.655147  0.576497  0.798449  0.669203  0.564517  1.000000   \n",
       "6            arm  0.560490  0.529558  0.518258  0.587157  0.602313  0.589304   \n",
       "7           hand  0.798085  0.703956  0.803878  0.763389  0.636055  0.745915   \n",
       "8           foot  0.773818  0.695968  0.672096  0.744822  0.691362  0.706871   \n",
       "9            eye  0.811312  0.797959  0.739001  0.761645  0.764978  0.678523   \n",
       "10     apartment  0.698721  0.762514  0.596658  0.621620  0.608525  0.445907   \n",
       "11         igloo  0.661859  0.688277  0.670116  0.640349  0.679977  0.570472   \n",
       "12          barn  0.711925  0.737495  0.622361  0.619353  0.651241  0.470127   \n",
       "13         house  0.635954  0.703997  0.529147  0.588550  0.651021  0.356240   \n",
       "14        church  0.632664  0.686622  0.568706  0.601965  0.621611  0.481075   \n",
       "15          door  0.664959  0.697948  0.707445  0.654743  0.614215  0.660156   \n",
       "16        closet  0.715603  0.687266  0.673670  0.612249  0.605650  0.643443   \n",
       "17       chimney  0.758883  0.759291  0.688216  0.698518  0.717023  0.673674   \n",
       "18        window  0.762504  0.718538  0.666203  0.622478  0.579022  0.610803   \n",
       "19          arch  0.610093  0.671840  0.491565  0.505957  0.506032  0.395335   \n",
       "20         shirt  0.878576  0.834256  0.813089  0.840423  0.777312  0.678780   \n",
       "21         pants  0.777598  0.736485  0.794516  0.776590  0.731372  0.805345   \n",
       "22         skirt  0.822725  0.790998  0.769078  0.799265  0.749963  0.688187   \n",
       "23          coat  0.889299  0.816334  0.796006  0.839576  0.808395  0.746119   \n",
       "24         dress  0.830805  0.784709  0.832871  0.818860  0.754664  0.778052   \n",
       "25         chair  0.768306  0.672516  0.747876  0.667374  0.600179  0.726622   \n",
       "26       dresser  0.788659  0.722362  0.595316  0.616838  0.617454  0.481023   \n",
       "27           bed  0.725328  0.751571  0.598146  0.701655  0.736045  0.547282   \n",
       "28         table  0.726813  0.676546  0.579006  0.631826  0.659164  0.607550   \n",
       "29          desk  0.684069  0.627647  0.501265  0.530369  0.567477  0.484901   \n",
       "30           ant  0.691065  0.777583  0.754469  0.800461  0.834809  0.636324   \n",
       "31     butterfly  0.738073  0.782226  0.704329  0.776244  0.729092  0.501622   \n",
       "32           fly  0.870237  0.852732  0.799885  0.828591  0.807930  0.631929   \n",
       "33           bee  0.902443  0.890533  0.875222  0.878088  0.826075  0.736508   \n",
       "34        beetle  0.857445  0.866394  0.785367  0.859938  0.809364  0.643236   \n",
       "35         knife  0.587953  0.616830  0.514735  0.576219  0.615567  0.544385   \n",
       "36        bottle  0.695385  0.644273  0.758616  0.664466  0.583204  0.768683   \n",
       "37         glass  0.669068  0.609779  0.716930  0.602703  0.465573  0.628759   \n",
       "38         spoon  0.689416  0.703103  0.557828  0.682571  0.697471  0.555059   \n",
       "39           cup  0.631058  0.672261  0.557587  0.678326  0.642021  0.523357   \n",
       "40     telephone  0.907903  0.809121  0.770225  0.807333  0.764701  0.724612   \n",
       "41           key  0.697340  0.703172  0.850677  0.734032  0.635849  0.761813   \n",
       "42         watch  0.756502  0.698777  0.678881  0.727094  0.746676  0.739074   \n",
       "43  refrigerator  0.745210  0.726670  0.698859  0.658273  0.641891  0.618369   \n",
       "44          bell  0.761587  0.770673  0.752112  0.704024  0.705145  0.596411   \n",
       "45   screwdriver  0.525716  0.512598  0.416282  0.524439  0.564325  0.562187   \n",
       "46        chisel  0.837904  0.714995  0.777330  0.718992  0.638073  0.789969   \n",
       "47        pliers  0.662072  0.683640  0.563572  0.703940  0.724202  0.587292   \n",
       "48           saw  0.638146  0.679344  0.538485  0.648948  0.691530  0.543095   \n",
       "49        hammer  0.556451  0.545172  0.433060  0.577171  0.636218  0.524448   \n",
       "50        tomato  0.679341  0.678013  0.571532  0.644053  0.561507  0.324236   \n",
       "51        celery  0.517919  0.514908  0.317849  0.475084  0.544593  0.241527   \n",
       "52          corn  0.809874  0.710625  0.771336  0.698746  0.655788  0.740154   \n",
       "53        carrot  0.586457  0.626575  0.417673  0.537472  0.623611  0.298281   \n",
       "54       lettuce  0.814620  0.799161  0.768428  0.784360  0.740062  0.635136   \n",
       "55       bicycle  0.886597  0.849823  0.835674  0.831094  0.792543  0.779629   \n",
       "56           car  0.591752  0.696179  0.513325  0.610807  0.700272  0.405805   \n",
       "57         train  0.698553  0.748991  0.669761  0.618325  0.650916  0.586700   \n",
       "58         truck  0.770612  0.761983  0.608832  0.649231  0.690289  0.483917   \n",
       "59      airplane  0.830323  0.828290  0.677270  0.724007  0.769298  0.521596   \n",
       "\n",
       "         arm      hand      foot  ...    tomato    celery      corn    carrot  \\\n",
       "0   0.560490  0.798085  0.773818  ...  0.679341  0.517919  0.809874  0.586457   \n",
       "1   0.529558  0.703956  0.695968  ...  0.678013  0.514908  0.710625  0.626575   \n",
       "2   0.518258  0.803878  0.672096  ...  0.571532  0.317849  0.771336  0.417673   \n",
       "3   0.587157  0.763389  0.744822  ...  0.644053  0.475084  0.698746  0.537472   \n",
       "4   0.602313  0.636055  0.691362  ...  0.561507  0.544593  0.655788  0.623611   \n",
       "5   0.589304  0.745915  0.706871  ...  0.324236  0.241527  0.740154  0.298281   \n",
       "6   1.000000  0.683080  0.846284  ...  0.211426  0.550446  0.497993  0.652083   \n",
       "7   0.683080  1.000000  0.840091  ...  0.528932  0.426071  0.744326  0.466841   \n",
       "8   0.846284  0.840091  1.000000  ...  0.449388  0.578857  0.695290  0.649962   \n",
       "9   0.614393  0.756584  0.762380  ...  0.518579  0.535730  0.842577  0.609926   \n",
       "10  0.231247  0.482260  0.448296  ...  0.587824  0.295193  0.649879  0.399071   \n",
       "11  0.403760  0.536716  0.530904  ...  0.415552  0.403774  0.759893  0.475588   \n",
       "12  0.356480  0.513918  0.526572  ...  0.560346  0.375587  0.661230  0.501225   \n",
       "13  0.172805  0.388721  0.360437  ...  0.512642  0.301658  0.615431  0.407944   \n",
       "14  0.309655  0.449913  0.464908  ...  0.460718  0.367576  0.673649  0.421551   \n",
       "15  0.389011  0.605931  0.573216  ...  0.484830  0.277677  0.765617  0.370305   \n",
       "16  0.464889  0.615073  0.621563  ...  0.392241  0.312676  0.751869  0.463871   \n",
       "17  0.513790  0.589217  0.669381  ...  0.451119  0.457083  0.772736  0.514505   \n",
       "18  0.376803  0.623167  0.589509  ...  0.586896  0.428371  0.784558  0.458079   \n",
       "19  0.280516  0.423684  0.435040  ...  0.525820  0.316577  0.620526  0.438808   \n",
       "20  0.494433  0.778000  0.727838  ...  0.741920  0.529024  0.819480  0.538137   \n",
       "21  0.596718  0.750308  0.754078  ...  0.492070  0.477946  0.858527  0.495714   \n",
       "22  0.486777  0.759455  0.678832  ...  0.587417  0.492523  0.792462  0.491346   \n",
       "23  0.645293  0.794849  0.811017  ...  0.641827  0.586026  0.854764  0.623968   \n",
       "24  0.627562  0.820303  0.787593  ...  0.583753  0.506032  0.829335  0.539895   \n",
       "25  0.494652  0.716269  0.686106  ...  0.506024  0.398247  0.847935  0.454849   \n",
       "26  0.452728  0.643746  0.639404  ...  0.565132  0.487753  0.785302  0.534382   \n",
       "27  0.465465  0.535473  0.619548  ...  0.475872  0.483395  0.695193  0.554463   \n",
       "28  0.646708  0.617186  0.754221  ...  0.428788  0.543479  0.757842  0.608505   \n",
       "29  0.432561  0.496075  0.557414  ...  0.384405  0.404495  0.699622  0.484119   \n",
       "30  0.635829  0.691927  0.659346  ...  0.411848  0.512513  0.619320  0.554288   \n",
       "31  0.300020  0.634290  0.525791  ...  0.728920  0.401224  0.703990  0.470192   \n",
       "32  0.632948  0.802939  0.762990  ...  0.607168  0.572116  0.743052  0.604350   \n",
       "33  0.567510  0.819981  0.758506  ...  0.677455  0.506895  0.844244  0.581631   \n",
       "34  0.531996  0.761997  0.725093  ...  0.669382  0.562920  0.776933  0.635357   \n",
       "35  0.702187  0.568099  0.705813  ...  0.413247  0.654923  0.611017  0.753891   \n",
       "36  0.539871  0.715812  0.694144  ...  0.493462  0.429526  0.832812  0.440403   \n",
       "37  0.303552  0.672058  0.564443  ...  0.618897  0.352579  0.770443  0.323041   \n",
       "38  0.722344  0.649860  0.789202  ...  0.513288  0.699333  0.613033  0.820611   \n",
       "39  0.439673  0.570131  0.596660  ...  0.680988  0.474785  0.656271  0.595870   \n",
       "40  0.574842  0.772525  0.773986  ...  0.645410  0.546342  0.818373  0.585782   \n",
       "41  0.451965  0.772869  0.624108  ...  0.563528  0.350159  0.824987  0.384281   \n",
       "42  0.644681  0.672264  0.781115  ...  0.424524  0.554331  0.811393  0.617313   \n",
       "43  0.334180  0.556853  0.545528  ...  0.590699  0.427098  0.799737  0.481697   \n",
       "44  0.459060  0.664241  0.624405  ...  0.633726  0.540109  0.809442  0.577536   \n",
       "45  0.744482  0.526077  0.747239  ...  0.276495  0.665802  0.544270  0.737703   \n",
       "46  0.624913  0.791892  0.802794  ...  0.569381  0.479680  0.851834  0.553871   \n",
       "47  0.602502  0.586857  0.710188  ...  0.526958  0.667040  0.687934  0.653829   \n",
       "48  0.712552  0.576657  0.746569  ...  0.490540  0.650495  0.628111  0.784718   \n",
       "49  0.717967  0.522666  0.717579  ...  0.360475  0.708623  0.579661  0.769853   \n",
       "50  0.211426  0.528932  0.449388  ...  1.000000  0.500313  0.561782  0.506339   \n",
       "51  0.550446  0.426071  0.578857  ...  0.500313  1.000000  0.475159  0.763209   \n",
       "52  0.497993  0.744326  0.695290  ...  0.561782  0.475159  1.000000  0.456373   \n",
       "53  0.652083  0.466841  0.649962  ...  0.506339  0.763209  0.456373  1.000000   \n",
       "54  0.436008  0.739569  0.644539  ...  0.678939  0.542782  0.824888  0.498010   \n",
       "55  0.698630  0.838479  0.824975  ...  0.560498  0.519049  0.818197  0.624687   \n",
       "56  0.389628  0.416598  0.479248  ...  0.386559  0.344661  0.574108  0.554217   \n",
       "57  0.465599  0.552607  0.553614  ...  0.380307  0.334831  0.708325  0.499876   \n",
       "58  0.566975  0.609400  0.694302  ...  0.442160  0.504676  0.667350  0.660626   \n",
       "59  0.611928  0.669398  0.709507  ...  0.541360  0.582738  0.699529  0.713950   \n",
       "\n",
       "     lettuce   bicycle       car     train     truck  airplane  \n",
       "0   0.814620  0.886597  0.591752  0.698553  0.770612  0.830323  \n",
       "1   0.799161  0.849823  0.696179  0.748991  0.761983  0.828290  \n",
       "2   0.768428  0.835674  0.513325  0.669761  0.608832  0.677270  \n",
       "3   0.784360  0.831094  0.610807  0.618325  0.649231  0.724007  \n",
       "4   0.740062  0.792543  0.700272  0.650916  0.690289  0.769298  \n",
       "5   0.635136  0.779629  0.405805  0.586700  0.483917  0.521596  \n",
       "6   0.436008  0.698630  0.389628  0.465599  0.566975  0.611928  \n",
       "7   0.739569  0.838479  0.416598  0.552607  0.609400  0.669398  \n",
       "8   0.644539  0.824975  0.479248  0.553614  0.694302  0.709507  \n",
       "9   0.822865  0.888571  0.742480  0.823249  0.822879  0.826608  \n",
       "10  0.730971  0.689024  0.722454  0.822750  0.710393  0.733315  \n",
       "11  0.739594  0.736073  0.753110  0.824186  0.746618  0.724672  \n",
       "12  0.706021  0.714799  0.758685  0.815631  0.817320  0.801912  \n",
       "13  0.699130  0.633268  0.788316  0.815476  0.734614  0.730749  \n",
       "14  0.727649  0.695612  0.714085  0.826501  0.705087  0.706253  \n",
       "15  0.743025  0.744535  0.694549  0.837039  0.686668  0.699127  \n",
       "16  0.706825  0.801271  0.740680  0.883244  0.787451  0.748007  \n",
       "17  0.763782  0.831779  0.743953  0.859954  0.767059  0.789290  \n",
       "18  0.786179  0.769819  0.670236  0.804625  0.735202  0.742122  \n",
       "19  0.636221  0.630249  0.692751  0.800162  0.693101  0.709135  \n",
       "20  0.886150  0.848812  0.582945  0.659794  0.675980  0.766515  \n",
       "21  0.809120  0.848815  0.570421  0.724547  0.629747  0.704129  \n",
       "22  0.854875  0.833014  0.564682  0.670851  0.626155  0.729970  \n",
       "23  0.839451  0.893446  0.635014  0.717671  0.736458  0.817678  \n",
       "24  0.828975  0.870090  0.535723  0.691407  0.642177  0.749929  \n",
       "25  0.789435  0.817605  0.583464  0.757182  0.698082  0.690255  \n",
       "26  0.770415  0.757685  0.676241  0.781657  0.799060  0.787166  \n",
       "27  0.728105  0.771604  0.761585  0.770276  0.746601  0.729726  \n",
       "28  0.666310  0.784173  0.713096  0.769848  0.788410  0.780613  \n",
       "29  0.669863  0.715910  0.739774  0.840375  0.809299  0.768639  \n",
       "30  0.689799  0.783339  0.598768  0.625851  0.623240  0.722793  \n",
       "31  0.834170  0.722425  0.615877  0.622989  0.567113  0.688224  \n",
       "32  0.785248  0.849898  0.539665  0.640618  0.728240  0.822813  \n",
       "33  0.852035  0.908592  0.646222  0.737953  0.734187  0.818343  \n",
       "34  0.844725  0.859050  0.635763  0.675059  0.692186  0.793873  \n",
       "35  0.540646  0.698375  0.509632  0.549013  0.565088  0.624987  \n",
       "36  0.768282  0.786941  0.502812  0.728353  0.628853  0.654319  \n",
       "37  0.776409  0.685610  0.410371  0.611967  0.541319  0.559989  \n",
       "38  0.604297  0.763830  0.599706  0.562963  0.663477  0.706858  \n",
       "39  0.635976  0.674283  0.683660  0.596926  0.604378  0.625303  \n",
       "40  0.810940  0.890905  0.608256  0.671767  0.714214  0.789244  \n",
       "41  0.794073  0.771416  0.495346  0.652303  0.568583  0.642873  \n",
       "42  0.738202  0.847350  0.704555  0.733981  0.741210  0.751133  \n",
       "43  0.798355  0.752779  0.663390  0.795322  0.687058  0.718935  \n",
       "44  0.854777  0.793306  0.618322  0.750792  0.703394  0.799112  \n",
       "45  0.457360  0.651132  0.429106  0.427948  0.497080  0.541481  \n",
       "46  0.740476  0.862722  0.499067  0.655145  0.669871  0.713600  \n",
       "47  0.670162  0.721488  0.545915  0.517871  0.557607  0.633766  \n",
       "48  0.575359  0.731213  0.642383  0.620108  0.665620  0.715041  \n",
       "49  0.512744  0.657830  0.538586  0.490798  0.572200  0.603538  \n",
       "50  0.678939  0.560498  0.386559  0.380307  0.442160  0.541360  \n",
       "51  0.542782  0.519049  0.344661  0.334831  0.504676  0.582738  \n",
       "52  0.824888  0.818197  0.574108  0.708325  0.667350  0.699529  \n",
       "53  0.498010  0.624687  0.554217  0.499876  0.660626  0.713950  \n",
       "54  1.000000  0.814918  0.565864  0.692537  0.664510  0.742056  \n",
       "55  0.814918  1.000000  0.655187  0.790419  0.787582  0.843646  \n",
       "56  0.565864  0.655187  1.000000  0.802225  0.798727  0.745591  \n",
       "57  0.692537  0.790419  0.802225  1.000000  0.829152  0.822606  \n",
       "58  0.664510  0.787582  0.798727  0.829152  1.000000  0.880917  \n",
       "59  0.742056  0.843646  0.745591  0.822606  0.880917  1.000000  \n",
       "\n",
       "[60 rows x 61 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "neural_sim_df_reordered"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "39126120",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iterate over all unique pairs of words\n",
    "for word1, word2 in combinations(words, 2):\n",
    "    # Extract vectors\n",
    "    word1_neural_vector = neural_sim_df_reordered[word1]\n",
    "    word2_neural_vector = neural_sim_df_reordered[word2]\n",
    "    word1_semantic_vector = word2vec_sim_df[word1]\n",
    "    word2_semantic_vector = word2vec_sim_df[word2]\n",
    "\n",
    "    # Remove indices with perfect correlations\n",
    "    perfect_corr_indices = (word1_neural_vector == 1.0) | (word2_neural_vector == 1.0) | \\\n",
    "                           (word1_semantic_vector == 1.0) | (word2_semantic_vector == 1.0)\n",
    "    word1_neural_vector_filtered = word1_neural_vector[~perfect_corr_indices]\n",
    "    word2_neural_vector_filtered = word2_neural_vector[~perfect_corr_indices]\n",
    "    word1_semantic_vector_filtered = word1_semantic_vector[~perfect_corr_indices]\n",
    "    word2_semantic_vector_filtered = word2_semantic_vector[~perfect_corr_indices]\n",
    "\n",
    "    # Calculate correlations\n",
    "    corr_word1_neural_semantic = pearsonr(word1_neural_vector_filtered, word1_semantic_vector_filtered)[0]\n",
    "    corr_word1_neural_word2_semantic = pearsonr(word1_neural_vector_filtered, word2_semantic_vector_filtered)[0]\n",
    "    corr_word2_neural_semantic = pearsonr(word2_neural_vector_filtered, word2_semantic_vector_filtered)[0]\n",
    "    corr_word2_neural_word1_semantic = pearsonr(word2_neural_vector_filtered, word1_semantic_vector_filtered)[0]\n",
    "\n",
    "    # Check decoding accuracy\n",
    "    decode_accuracy_word1 = corr_word1_neural_semantic > corr_word1_neural_word2_semantic\n",
    "    decode_accuracy_word2 = corr_word2_neural_semantic > corr_word2_neural_word1_semantic\n",
    "\n",
    "    # Append results\n",
    "    decoding_results.append({\n",
    "        'word1': word1,\n",
    "        'word2': word2,\n",
    "        'corr_word1_neural_semantic': corr_word1_neural_semantic,\n",
    "        'corr_word1_neural_word2_semantic': corr_word1_neural_word2_semantic,\n",
    "        'corr_word2_neural_semantic': corr_word2_neural_semantic,\n",
    "        'corr_word2_neural_word1_semantic': corr_word2_neural_word1_semantic,\n",
    "        'decode_accuracy_word1': decode_accuracy_word1,\n",
    "        'decode_accuracy_word2': decode_accuracy_word2\n",
    "    })\n",
    "\n",
    "# Convert results to a DataFrame\n",
    "decoding_results_df = pd.DataFrame(decoding_results)\n",
    "\n",
    "# Save the results to a CSV file\n",
    "#decoding_results_df.to_csv('decoding_results.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3e76422c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>word1</th>\n",
       "      <th>word2</th>\n",
       "      <th>corr_word1_neural_semantic</th>\n",
       "      <th>corr_word1_neural_word2_semantic</th>\n",
       "      <th>corr_word2_neural_semantic</th>\n",
       "      <th>corr_word2_neural_word1_semantic</th>\n",
       "      <th>decode_accuracy_word1</th>\n",
       "      <th>decode_accuracy_word2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>horse</td>\n",
       "      <td>bear</td>\n",
       "      <td>0.167891</td>\n",
       "      <td>0.155666</td>\n",
       "      <td>0.144818</td>\n",
       "      <td>0.143094</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>horse</td>\n",
       "      <td>cat</td>\n",
       "      <td>0.153609</td>\n",
       "      <td>0.135756</td>\n",
       "      <td>0.060013</td>\n",
       "      <td>0.162162</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>horse</td>\n",
       "      <td>dog</td>\n",
       "      <td>0.149693</td>\n",
       "      <td>0.081105</td>\n",
       "      <td>-0.098696</td>\n",
       "      <td>0.028098</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>horse</td>\n",
       "      <td>cow</td>\n",
       "      <td>0.169983</td>\n",
       "      <td>-0.037443</td>\n",
       "      <td>-0.070567</td>\n",
       "      <td>0.045873</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>horse</td>\n",
       "      <td>leg</td>\n",
       "      <td>0.148082</td>\n",
       "      <td>0.191753</td>\n",
       "      <td>0.143401</td>\n",
       "      <td>0.169095</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1765</th>\n",
       "      <td>car</td>\n",
       "      <td>truck</td>\n",
       "      <td>0.052116</td>\n",
       "      <td>0.372079</td>\n",
       "      <td>0.385075</td>\n",
       "      <td>0.095510</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1766</th>\n",
       "      <td>car</td>\n",
       "      <td>airplane</td>\n",
       "      <td>0.013257</td>\n",
       "      <td>0.438265</td>\n",
       "      <td>0.410750</td>\n",
       "      <td>0.112438</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1767</th>\n",
       "      <td>train</td>\n",
       "      <td>truck</td>\n",
       "      <td>0.418073</td>\n",
       "      <td>0.394883</td>\n",
       "      <td>0.355418</td>\n",
       "      <td>0.427436</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1768</th>\n",
       "      <td>train</td>\n",
       "      <td>airplane</td>\n",
       "      <td>0.415714</td>\n",
       "      <td>0.472863</td>\n",
       "      <td>0.404492</td>\n",
       "      <td>0.463016</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1769</th>\n",
       "      <td>truck</td>\n",
       "      <td>airplane</td>\n",
       "      <td>0.345542</td>\n",
       "      <td>0.410639</td>\n",
       "      <td>0.407380</td>\n",
       "      <td>0.366215</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1770 rows × 8 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      word1     word2  corr_word1_neural_semantic  \\\n",
       "0     horse      bear                    0.167891   \n",
       "1     horse       cat                    0.153609   \n",
       "2     horse       dog                    0.149693   \n",
       "3     horse       cow                    0.169983   \n",
       "4     horse       leg                    0.148082   \n",
       "...     ...       ...                         ...   \n",
       "1765    car     truck                    0.052116   \n",
       "1766    car  airplane                    0.013257   \n",
       "1767  train     truck                    0.418073   \n",
       "1768  train  airplane                    0.415714   \n",
       "1769  truck  airplane                    0.345542   \n",
       "\n",
       "      corr_word1_neural_word2_semantic  corr_word2_neural_semantic  \\\n",
       "0                             0.155666                    0.144818   \n",
       "1                             0.135756                    0.060013   \n",
       "2                             0.081105                   -0.098696   \n",
       "3                            -0.037443                   -0.070567   \n",
       "4                             0.191753                    0.143401   \n",
       "...                                ...                         ...   \n",
       "1765                          0.372079                    0.385075   \n",
       "1766                          0.438265                    0.410750   \n",
       "1767                          0.394883                    0.355418   \n",
       "1768                          0.472863                    0.404492   \n",
       "1769                          0.410639                    0.407380   \n",
       "\n",
       "      corr_word2_neural_word1_semantic  decode_accuracy_word1  \\\n",
       "0                             0.143094                   True   \n",
       "1                             0.162162                   True   \n",
       "2                             0.028098                   True   \n",
       "3                             0.045873                   True   \n",
       "4                             0.169095                  False   \n",
       "...                                ...                    ...   \n",
       "1765                          0.095510                  False   \n",
       "1766                          0.112438                  False   \n",
       "1767                          0.427436                   True   \n",
       "1768                          0.463016                  False   \n",
       "1769                          0.366215                  False   \n",
       "\n",
       "      decode_accuracy_word2  \n",
       "0                      True  \n",
       "1                     False  \n",
       "2                     False  \n",
       "3                     False  \n",
       "4                     False  \n",
       "...                     ...  \n",
       "1765                   True  \n",
       "1766                   True  \n",
       "1767                  False  \n",
       "1768                  False  \n",
       "1769                   True  \n",
       "\n",
       "[1770 rows x 8 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "decoding_results_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "313746bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the overall decoder accuracy from the results DataFrame\n",
    "total_accuracy = (decoding_results_df['decode_accuracy_word1'].sum() + \n",
    "                  decoding_results_df['decode_accuracy_word2'].sum())\n",
    "p1_overall_accuracy = total_accuracy / (2 * len(decoding_results_df))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "aeb29950",
   "metadata": {},
   "outputs": [],
   "source": [
    "neural_sim_df = pd.read_csv('p2_sim.csv')\n",
    "\n",
    "#Align the columns of the neural similarity matrix to match the word2vec similarity matrix\n",
    "neural_sim_df_aligned = neural_sim_df[word2vec_sim_df.columns]\n",
    "\n",
    "# Reorder the rows of the neural similarity matrix to match the order in the word2vec similarity matrix\n",
    "neural_sim_df_reordered = neural_sim_df_aligned.set_index('term').reindex(word2vec_sim_df['term']).reset_index()\n",
    "# List of words (excluding the 'term' column)\n",
    "words = word2vec_sim_df.columns[1:]\n",
    "\n",
    "# Initialize a list to store results\n",
    "decoding_results = []\n",
    "\n",
    "# Iterate over all unique pairs of words\n",
    "for word1, word2 in combinations(words, 2):\n",
    "    # Extract vectors\n",
    "    word1_neural_vector = neural_sim_df_reordered[word1]\n",
    "    word2_neural_vector = neural_sim_df_reordered[word2]\n",
    "    word1_semantic_vector = word2vec_sim_df[word1]\n",
    "    word2_semantic_vector = word2vec_sim_df[word2]\n",
    "\n",
    "    # Remove indices with perfect correlations\n",
    "    perfect_corr_indices = (word1_neural_vector == 1.0) | (word2_neural_vector == 1.0) | \\\n",
    "                           (word1_semantic_vector == 1.0) | (word2_semantic_vector == 1.0)\n",
    "    word1_neural_vector_filtered = word1_neural_vector[~perfect_corr_indices]\n",
    "    word2_neural_vector_filtered = word2_neural_vector[~perfect_corr_indices]\n",
    "    word1_semantic_vector_filtered = word1_semantic_vector[~perfect_corr_indices]\n",
    "    word2_semantic_vector_filtered = word2_semantic_vector[~perfect_corr_indices]\n",
    "\n",
    "    # Calculate correlations\n",
    "    corr_word1_neural_semantic = pearsonr(word1_neural_vector_filtered, word1_semantic_vector_filtered)[0]\n",
    "    corr_word1_neural_word2_semantic = pearsonr(word1_neural_vector_filtered, word2_semantic_vector_filtered)[0]\n",
    "    corr_word2_neural_semantic = pearsonr(word2_neural_vector_filtered, word2_semantic_vector_filtered)[0]\n",
    "    corr_word2_neural_word1_semantic = pearsonr(word2_neural_vector_filtered, word1_semantic_vector_filtered)[0]\n",
    "\n",
    "    # Check decoding accuracy\n",
    "    decode_accuracy_word1 = corr_word1_neural_semantic > corr_word1_neural_word2_semantic\n",
    "    decode_accuracy_word2 = corr_word2_neural_semantic > corr_word2_neural_word1_semantic\n",
    "\n",
    "    # Append results\n",
    "    decoding_results.append({\n",
    "        'word1': word1,\n",
    "        'word2': word2,\n",
    "        'corr_word1_neural_semantic': corr_word1_neural_semantic,\n",
    "        'corr_word1_neural_word2_semantic': corr_word1_neural_word2_semantic,\n",
    "        'corr_word2_neural_semantic': corr_word2_neural_semantic,\n",
    "        'corr_word2_neural_word1_semantic': corr_word2_neural_word1_semantic,\n",
    "        'decode_accuracy_word1': decode_accuracy_word1,\n",
    "        'decode_accuracy_word2': decode_accuracy_word2\n",
    "    })\n",
    "\n",
    "# Convert results to a DataFrame\n",
    "decoding_results_df = pd.DataFrame(decoding_results)\n",
    "\n",
    "# Calculate the overall decoder accuracy from the results DataFrame\n",
    "total_accuracy = (decoding_results_df['decode_accuracy_word1'].sum() + \n",
    "                  decoding_results_df['decode_accuracy_word2'].sum())\n",
    "p2_overall_accuracy = total_accuracy / (2 * len(decoding_results_df))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1f89c966",
   "metadata": {},
   "outputs": [],
   "source": [
    "neural_sim_df = pd.read_csv('p3_sim.csv')\n",
    "\n",
    "#Align the columns of the neural similarity matrix to match the word2vec similarity matrix\n",
    "neural_sim_df_aligned = neural_sim_df[word2vec_sim_df.columns]\n",
    "\n",
    "# Reorder the rows of the neural similarity matrix to match the order in the word2vec similarity matrix\n",
    "neural_sim_df_reordered = neural_sim_df_aligned.set_index('term').reindex(word2vec_sim_df['term']).reset_index()\n",
    "# List of words (excluding the 'term' column)\n",
    "words = word2vec_sim_df.columns[1:]\n",
    "\n",
    "# Initialize a list to store results\n",
    "decoding_results = []\n",
    "\n",
    "# Iterate over all unique pairs of words\n",
    "for word1, word2 in combinations(words, 2):\n",
    "    # Extract vectors\n",
    "    word1_neural_vector = neural_sim_df_reordered[word1]\n",
    "    word2_neural_vector = neural_sim_df_reordered[word2]\n",
    "    word1_semantic_vector = word2vec_sim_df[word1]\n",
    "    word2_semantic_vector = word2vec_sim_df[word2]\n",
    "\n",
    "    # Remove indices with perfect correlations\n",
    "    perfect_corr_indices = (word1_neural_vector == 1.0) | (word2_neural_vector == 1.0) | \\\n",
    "                           (word1_semantic_vector == 1.0) | (word2_semantic_vector == 1.0)\n",
    "    word1_neural_vector_filtered = word1_neural_vector[~perfect_corr_indices]\n",
    "    word2_neural_vector_filtered = word2_neural_vector[~perfect_corr_indices]\n",
    "    word1_semantic_vector_filtered = word1_semantic_vector[~perfect_corr_indices]\n",
    "    word2_semantic_vector_filtered = word2_semantic_vector[~perfect_corr_indices]\n",
    "\n",
    "    # Calculate correlations\n",
    "    corr_word1_neural_semantic = pearsonr(word1_neural_vector_filtered, word1_semantic_vector_filtered)[0]\n",
    "    corr_word1_neural_word2_semantic = pearsonr(word1_neural_vector_filtered, word2_semantic_vector_filtered)[0]\n",
    "    corr_word2_neural_semantic = pearsonr(word2_neural_vector_filtered, word2_semantic_vector_filtered)[0]\n",
    "    corr_word2_neural_word1_semantic = pearsonr(word2_neural_vector_filtered, word1_semantic_vector_filtered)[0]\n",
    "\n",
    "    # Check decoding accuracy\n",
    "    decode_accuracy_word1 = corr_word1_neural_semantic > corr_word1_neural_word2_semantic\n",
    "    decode_accuracy_word2 = corr_word2_neural_semantic > corr_word2_neural_word1_semantic\n",
    "\n",
    "    # Append results\n",
    "    decoding_results.append({\n",
    "        'word1': word1,\n",
    "        'word2': word2,\n",
    "        'corr_word1_neural_semantic': corr_word1_neural_semantic,\n",
    "        'corr_word1_neural_word2_semantic': corr_word1_neural_word2_semantic,\n",
    "        'corr_word2_neural_semantic': corr_word2_neural_semantic,\n",
    "        'corr_word2_neural_word1_semantic': corr_word2_neural_word1_semantic,\n",
    "        'decode_accuracy_word1': decode_accuracy_word1,\n",
    "        'decode_accuracy_word2': decode_accuracy_word2\n",
    "    })\n",
    "\n",
    "# Convert results to a DataFrame\n",
    "decoding_results_df = pd.DataFrame(decoding_results)\n",
    "\n",
    "# Calculate the overall decoder accuracy from the results DataFrame\n",
    "total_accuracy = (decoding_results_df['decode_accuracy_word1'].sum() + \n",
    "                  decoding_results_df['decode_accuracy_word2'].sum())\n",
    "p3_overall_accuracy = total_accuracy / (2 * len(decoding_results_df))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c37bff6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "neural_sim_df = pd.read_csv('p4_sim.csv')\n",
    "\n",
    "#Align the columns of the neural similarity matrix to match the word2vec similarity matrix\n",
    "neural_sim_df_aligned = neural_sim_df[word2vec_sim_df.columns]\n",
    "\n",
    "# Reorder the rows of the neural similarity matrix to match the order in the word2vec similarity matrix\n",
    "neural_sim_df_reordered = neural_sim_df_aligned.set_index('term').reindex(word2vec_sim_df['term']).reset_index()\n",
    "# List of words (excluding the 'term' column)\n",
    "words = word2vec_sim_df.columns[1:]\n",
    "\n",
    "# Initialize a list to store results\n",
    "decoding_results = []\n",
    "\n",
    "# Iterate over all unique pairs of words\n",
    "for word1, word2 in combinations(words, 2):\n",
    "    # Extract vectors\n",
    "    word1_neural_vector = neural_sim_df_reordered[word1]\n",
    "    word2_neural_vector = neural_sim_df_reordered[word2]\n",
    "    word1_semantic_vector = word2vec_sim_df[word1]\n",
    "    word2_semantic_vector = word2vec_sim_df[word2]\n",
    "\n",
    "    # Remove indices with perfect correlations\n",
    "    perfect_corr_indices = (word1_neural_vector == 1.0) | (word2_neural_vector == 1.0) | \\\n",
    "                           (word1_semantic_vector == 1.0) | (word2_semantic_vector == 1.0)\n",
    "    word1_neural_vector_filtered = word1_neural_vector[~perfect_corr_indices]\n",
    "    word2_neural_vector_filtered = word2_neural_vector[~perfect_corr_indices]\n",
    "    word1_semantic_vector_filtered = word1_semantic_vector[~perfect_corr_indices]\n",
    "    word2_semantic_vector_filtered = word2_semantic_vector[~perfect_corr_indices]\n",
    "\n",
    "    # Calculate correlations\n",
    "    corr_word1_neural_semantic = pearsonr(word1_neural_vector_filtered, word1_semantic_vector_filtered)[0]\n",
    "    corr_word1_neural_word2_semantic = pearsonr(word1_neural_vector_filtered, word2_semantic_vector_filtered)[0]\n",
    "    corr_word2_neural_semantic = pearsonr(word2_neural_vector_filtered, word2_semantic_vector_filtered)[0]\n",
    "    corr_word2_neural_word1_semantic = pearsonr(word2_neural_vector_filtered, word1_semantic_vector_filtered)[0]\n",
    "\n",
    "    # Check decoding accuracy\n",
    "    decode_accuracy_word1 = corr_word1_neural_semantic > corr_word1_neural_word2_semantic\n",
    "    decode_accuracy_word2 = corr_word2_neural_semantic > corr_word2_neural_word1_semantic\n",
    "\n",
    "    # Append results\n",
    "    decoding_results.append({\n",
    "        'word1': word1,\n",
    "        'word2': word2,\n",
    "        'corr_word1_neural_semantic': corr_word1_neural_semantic,\n",
    "        'corr_word1_neural_word2_semantic': corr_word1_neural_word2_semantic,\n",
    "        'corr_word2_neural_semantic': corr_word2_neural_semantic,\n",
    "        'corr_word2_neural_word1_semantic': corr_word2_neural_word1_semantic,\n",
    "        'decode_accuracy_word1': decode_accuracy_word1,\n",
    "        'decode_accuracy_word2': decode_accuracy_word2\n",
    "    })\n",
    "\n",
    "# Convert results to a DataFrame\n",
    "decoding_results_df = pd.DataFrame(decoding_results)\n",
    "\n",
    "# Calculate the overall decoder accuracy from the results DataFrame\n",
    "total_accuracy = (decoding_results_df['decode_accuracy_word1'].sum() + \n",
    "                  decoding_results_df['decode_accuracy_word2'].sum())\n",
    "p4_overall_accuracy = total_accuracy / (2 * len(decoding_results_df))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9ecf41dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "neural_sim_df = pd.read_csv('p5_sim.csv')\n",
    "\n",
    "#Align the columns of the neural similarity matrix to match the word2vec similarity matrix\n",
    "neural_sim_df_aligned = neural_sim_df[word2vec_sim_df.columns]\n",
    "\n",
    "# Reorder the rows of the neural similarity matrix to match the order in the word2vec similarity matrix\n",
    "neural_sim_df_reordered = neural_sim_df_aligned.set_index('term').reindex(word2vec_sim_df['term']).reset_index()\n",
    "# List of words (excluding the 'term' column)\n",
    "words = word2vec_sim_df.columns[1:]\n",
    "\n",
    "# Initialize a list to store results\n",
    "decoding_results = []\n",
    "\n",
    "# Iterate over all unique pairs of words\n",
    "for word1, word2 in combinations(words, 2):\n",
    "    # Extract vectors\n",
    "    word1_neural_vector = neural_sim_df_reordered[word1]\n",
    "    word2_neural_vector = neural_sim_df_reordered[word2]\n",
    "    word1_semantic_vector = word2vec_sim_df[word1]\n",
    "    word2_semantic_vector = word2vec_sim_df[word2]\n",
    "\n",
    "    # Remove indices with perfect correlations\n",
    "    perfect_corr_indices = (word1_neural_vector == 1.0) | (word2_neural_vector == 1.0) | \\\n",
    "                           (word1_semantic_vector == 1.0) | (word2_semantic_vector == 1.0)\n",
    "    word1_neural_vector_filtered = word1_neural_vector[~perfect_corr_indices]\n",
    "    word2_neural_vector_filtered = word2_neural_vector[~perfect_corr_indices]\n",
    "    word1_semantic_vector_filtered = word1_semantic_vector[~perfect_corr_indices]\n",
    "    word2_semantic_vector_filtered = word2_semantic_vector[~perfect_corr_indices]\n",
    "\n",
    "    # Calculate correlations\n",
    "    corr_word1_neural_semantic = pearsonr(word1_neural_vector_filtered, word1_semantic_vector_filtered)[0]\n",
    "    corr_word1_neural_word2_semantic = pearsonr(word1_neural_vector_filtered, word2_semantic_vector_filtered)[0]\n",
    "    corr_word2_neural_semantic = pearsonr(word2_neural_vector_filtered, word2_semantic_vector_filtered)[0]\n",
    "    corr_word2_neural_word1_semantic = pearsonr(word2_neural_vector_filtered, word1_semantic_vector_filtered)[0]\n",
    "\n",
    "    # Check decoding accuracy\n",
    "    decode_accuracy_word1 = corr_word1_neural_semantic > corr_word1_neural_word2_semantic\n",
    "    decode_accuracy_word2 = corr_word2_neural_semantic > corr_word2_neural_word1_semantic\n",
    "\n",
    "    # Append results\n",
    "    decoding_results.append({\n",
    "        'word1': word1,\n",
    "        'word2': word2,\n",
    "        'corr_word1_neural_semantic': corr_word1_neural_semantic,\n",
    "        'corr_word1_neural_word2_semantic': corr_word1_neural_word2_semantic,\n",
    "        'corr_word2_neural_semantic': corr_word2_neural_semantic,\n",
    "        'corr_word2_neural_word1_semantic': corr_word2_neural_word1_semantic,\n",
    "        'decode_accuracy_word1': decode_accuracy_word1,\n",
    "        'decode_accuracy_word2': decode_accuracy_word2\n",
    "    })\n",
    "\n",
    "# Convert results to a DataFrame\n",
    "decoding_results_df = pd.DataFrame(decoding_results)\n",
    "\n",
    "# Calculate the overall decoder accuracy from the results DataFrame\n",
    "total_accuracy = (decoding_results_df['decode_accuracy_word1'].sum() + \n",
    "                  decoding_results_df['decode_accuracy_word2'].sum())\n",
    "p5_overall_accuracy = total_accuracy / (2 * len(decoding_results_df))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ccc2566e",
   "metadata": {},
   "outputs": [],
   "source": [
    "neural_sim_df = pd.read_csv('p6_sim.csv')\n",
    "\n",
    "#Align the columns of the neural similarity matrix to match the word2vec similarity matrix\n",
    "neural_sim_df_aligned = neural_sim_df[word2vec_sim_df.columns]\n",
    "\n",
    "# Reorder the rows of the neural similarity matrix to match the order in the word2vec similarity matrix\n",
    "neural_sim_df_reordered = neural_sim_df_aligned.set_index('term').reindex(word2vec_sim_df['term']).reset_index()\n",
    "# List of words (excluding the 'term' column)\n",
    "words = word2vec_sim_df.columns[1:]\n",
    "\n",
    "# Initialize a list to store results\n",
    "decoding_results = []\n",
    "\n",
    "# Iterate over all unique pairs of words\n",
    "for word1, word2 in combinations(words, 2):\n",
    "    # Extract vectors\n",
    "    word1_neural_vector = neural_sim_df_reordered[word1]\n",
    "    word2_neural_vector = neural_sim_df_reordered[word2]\n",
    "    word1_semantic_vector = word2vec_sim_df[word1]\n",
    "    word2_semantic_vector = word2vec_sim_df[word2]\n",
    "\n",
    "    # Remove indices with perfect correlations\n",
    "    perfect_corr_indices = (word1_neural_vector == 1.0) | (word2_neural_vector == 1.0) | \\\n",
    "                           (word1_semantic_vector == 1.0) | (word2_semantic_vector == 1.0)\n",
    "    word1_neural_vector_filtered = word1_neural_vector[~perfect_corr_indices]\n",
    "    word2_neural_vector_filtered = word2_neural_vector[~perfect_corr_indices]\n",
    "    word1_semantic_vector_filtered = word1_semantic_vector[~perfect_corr_indices]\n",
    "    word2_semantic_vector_filtered = word2_semantic_vector[~perfect_corr_indices]\n",
    "\n",
    "    # Calculate correlations\n",
    "    corr_word1_neural_semantic = pearsonr(word1_neural_vector_filtered, word1_semantic_vector_filtered)[0]\n",
    "    corr_word1_neural_word2_semantic = pearsonr(word1_neural_vector_filtered, word2_semantic_vector_filtered)[0]\n",
    "    corr_word2_neural_semantic = pearsonr(word2_neural_vector_filtered, word2_semantic_vector_filtered)[0]\n",
    "    corr_word2_neural_word1_semantic = pearsonr(word2_neural_vector_filtered, word1_semantic_vector_filtered)[0]\n",
    "\n",
    "    # Check decoding accuracy\n",
    "    decode_accuracy_word1 = corr_word1_neural_semantic > corr_word1_neural_word2_semantic\n",
    "    decode_accuracy_word2 = corr_word2_neural_semantic > corr_word2_neural_word1_semantic\n",
    "\n",
    "    # Append results\n",
    "    decoding_results.append({\n",
    "        'word1': word1,\n",
    "        'word2': word2,\n",
    "        'corr_word1_neural_semantic': corr_word1_neural_semantic,\n",
    "        'corr_word1_neural_word2_semantic': corr_word1_neural_word2_semantic,\n",
    "        'corr_word2_neural_semantic': corr_word2_neural_semantic,\n",
    "        'corr_word2_neural_word1_semantic': corr_word2_neural_word1_semantic,\n",
    "        'decode_accuracy_word1': decode_accuracy_word1,\n",
    "        'decode_accuracy_word2': decode_accuracy_word2\n",
    "    })\n",
    "\n",
    "# Convert results to a DataFrame\n",
    "decoding_results_df = pd.DataFrame(decoding_results)\n",
    "\n",
    "# Calculate the overall decoder accuracy from the results DataFrame\n",
    "total_accuracy = (decoding_results_df['decode_accuracy_word1'].sum() + \n",
    "                  decoding_results_df['decode_accuracy_word2'].sum())\n",
    "p6_overall_accuracy = total_accuracy / (2 * len(decoding_results_df))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9bbf7776",
   "metadata": {},
   "outputs": [],
   "source": [
    "neural_sim_df = pd.read_csv('p7_sim.csv')\n",
    "\n",
    "#Align the columns of the neural similarity matrix to match the word2vec similarity matrix\n",
    "neural_sim_df_aligned = neural_sim_df[word2vec_sim_df.columns]\n",
    "\n",
    "# Reorder the rows of the neural similarity matrix to match the order in the word2vec similarity matrix\n",
    "neural_sim_df_reordered = neural_sim_df_aligned.set_index('term').reindex(word2vec_sim_df['term']).reset_index()\n",
    "# List of words (excluding the 'term' column)\n",
    "words = word2vec_sim_df.columns[1:]\n",
    "\n",
    "# Initialize a list to store results\n",
    "decoding_results = []\n",
    "\n",
    "# Iterate over all unique pairs of words\n",
    "for word1, word2 in combinations(words, 2):\n",
    "    # Extract vectors\n",
    "    word1_neural_vector = neural_sim_df_reordered[word1]\n",
    "    word2_neural_vector = neural_sim_df_reordered[word2]\n",
    "    word1_semantic_vector = word2vec_sim_df[word1]\n",
    "    word2_semantic_vector = word2vec_sim_df[word2]\n",
    "\n",
    "    # Remove indices with perfect correlations\n",
    "    perfect_corr_indices = (word1_neural_vector == 1.0) | (word2_neural_vector == 1.0) | \\\n",
    "                           (word1_semantic_vector == 1.0) | (word2_semantic_vector == 1.0)\n",
    "    word1_neural_vector_filtered = word1_neural_vector[~perfect_corr_indices]\n",
    "    word2_neural_vector_filtered = word2_neural_vector[~perfect_corr_indices]\n",
    "    word1_semantic_vector_filtered = word1_semantic_vector[~perfect_corr_indices]\n",
    "    word2_semantic_vector_filtered = word2_semantic_vector[~perfect_corr_indices]\n",
    "\n",
    "    # Calculate correlations\n",
    "    corr_word1_neural_semantic = pearsonr(word1_neural_vector_filtered, word1_semantic_vector_filtered)[0]\n",
    "    corr_word1_neural_word2_semantic = pearsonr(word1_neural_vector_filtered, word2_semantic_vector_filtered)[0]\n",
    "    corr_word2_neural_semantic = pearsonr(word2_neural_vector_filtered, word2_semantic_vector_filtered)[0]\n",
    "    corr_word2_neural_word1_semantic = pearsonr(word2_neural_vector_filtered, word1_semantic_vector_filtered)[0]\n",
    "\n",
    "    # Check decoding accuracy\n",
    "    decode_accuracy_word1 = corr_word1_neural_semantic > corr_word1_neural_word2_semantic\n",
    "    decode_accuracy_word2 = corr_word2_neural_semantic > corr_word2_neural_word1_semantic\n",
    "\n",
    "    # Append results\n",
    "    decoding_results.append({\n",
    "        'word1': word1,\n",
    "        'word2': word2,\n",
    "        'corr_word1_neural_semantic': corr_word1_neural_semantic,\n",
    "        'corr_word1_neural_word2_semantic': corr_word1_neural_word2_semantic,\n",
    "        'corr_word2_neural_semantic': corr_word2_neural_semantic,\n",
    "        'corr_word2_neural_word1_semantic': corr_word2_neural_word1_semantic,\n",
    "        'decode_accuracy_word1': decode_accuracy_word1,\n",
    "        'decode_accuracy_word2': decode_accuracy_word2\n",
    "    })\n",
    "\n",
    "# Convert results to a DataFrame\n",
    "decoding_results_df = pd.DataFrame(decoding_results)\n",
    "\n",
    "# Calculate the overall decoder accuracy from the results DataFrame\n",
    "total_accuracy = (decoding_results_df['decode_accuracy_word1'].sum() + \n",
    "                  decoding_results_df['decode_accuracy_word2'].sum())\n",
    "p7_overall_accuracy = total_accuracy / (2 * len(decoding_results_df))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "16faba74",
   "metadata": {},
   "outputs": [],
   "source": [
    "neural_sim_df = pd.read_csv('p8_sim.csv')\n",
    "\n",
    "#Align the columns of the neural similarity matrix to match the word2vec similarity matrix\n",
    "neural_sim_df_aligned = neural_sim_df[word2vec_sim_df.columns]\n",
    "\n",
    "# Reorder the rows of the neural similarity matrix to match the order in the word2vec similarity matrix\n",
    "neural_sim_df_reordered = neural_sim_df_aligned.set_index('term').reindex(word2vec_sim_df['term']).reset_index()\n",
    "# List of words (excluding the 'term' column)\n",
    "words = word2vec_sim_df.columns[1:]\n",
    "\n",
    "# Initialize a list to store results\n",
    "decoding_results = []\n",
    "\n",
    "# Iterate over all unique pairs of words\n",
    "for word1, word2 in combinations(words, 2):\n",
    "    # Extract vectors\n",
    "    word1_neural_vector = neural_sim_df_reordered[word1]\n",
    "    word2_neural_vector = neural_sim_df_reordered[word2]\n",
    "    word1_semantic_vector = word2vec_sim_df[word1]\n",
    "    word2_semantic_vector = word2vec_sim_df[word2]\n",
    "\n",
    "    # Remove indices with perfect correlations\n",
    "    perfect_corr_indices = (word1_neural_vector == 1.0) | (word2_neural_vector == 1.0) | \\\n",
    "                           (word1_semantic_vector == 1.0) | (word2_semantic_vector == 1.0)\n",
    "    word1_neural_vector_filtered = word1_neural_vector[~perfect_corr_indices]\n",
    "    word2_neural_vector_filtered = word2_neural_vector[~perfect_corr_indices]\n",
    "    word1_semantic_vector_filtered = word1_semantic_vector[~perfect_corr_indices]\n",
    "    word2_semantic_vector_filtered = word2_semantic_vector[~perfect_corr_indices]\n",
    "\n",
    "    # Calculate correlations\n",
    "    corr_word1_neural_semantic = pearsonr(word1_neural_vector_filtered, word1_semantic_vector_filtered)[0]\n",
    "    corr_word1_neural_word2_semantic = pearsonr(word1_neural_vector_filtered, word2_semantic_vector_filtered)[0]\n",
    "    corr_word2_neural_semantic = pearsonr(word2_neural_vector_filtered, word2_semantic_vector_filtered)[0]\n",
    "    corr_word2_neural_word1_semantic = pearsonr(word2_neural_vector_filtered, word1_semantic_vector_filtered)[0]\n",
    "\n",
    "    # Check decoding accuracy\n",
    "    decode_accuracy_word1 = corr_word1_neural_semantic > corr_word1_neural_word2_semantic\n",
    "    decode_accuracy_word2 = corr_word2_neural_semantic > corr_word2_neural_word1_semantic\n",
    "\n",
    "    # Append results\n",
    "    decoding_results.append({\n",
    "        'word1': word1,\n",
    "        'word2': word2,\n",
    "        'corr_word1_neural_semantic': corr_word1_neural_semantic,\n",
    "        'corr_word1_neural_word2_semantic': corr_word1_neural_word2_semantic,\n",
    "        'corr_word2_neural_semantic': corr_word2_neural_semantic,\n",
    "        'corr_word2_neural_word1_semantic': corr_word2_neural_word1_semantic,\n",
    "        'decode_accuracy_word1': decode_accuracy_word1,\n",
    "        'decode_accuracy_word2': decode_accuracy_word2\n",
    "    })\n",
    "\n",
    "# Convert results to a DataFrame\n",
    "decoding_results_df = pd.DataFrame(decoding_results)\n",
    "\n",
    "# Calculate the overall decoder accuracy from the results DataFrame\n",
    "total_accuracy = (decoding_results_df['decode_accuracy_word1'].sum() + \n",
    "                  decoding_results_df['decode_accuracy_word2'].sum())\n",
    "p8_overall_accuracy = total_accuracy / (2 * len(decoding_results_df))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "21f44522",
   "metadata": {},
   "outputs": [],
   "source": [
    "neural_sim_df = pd.read_csv('p9_sim.csv')\n",
    "\n",
    "#Align the columns of the neural similarity matrix to match the word2vec similarity matrix\n",
    "neural_sim_df_aligned = neural_sim_df[word2vec_sim_df.columns]\n",
    "\n",
    "# Reorder the rows of the neural similarity matrix to match the order in the word2vec similarity matrix\n",
    "neural_sim_df_reordered = neural_sim_df_aligned.set_index('term').reindex(word2vec_sim_df['term']).reset_index()\n",
    "# List of words (excluding the 'term' column)\n",
    "words = word2vec_sim_df.columns[1:]\n",
    "\n",
    "# Initialize a list to store results\n",
    "decoding_results = []\n",
    "\n",
    "# Iterate over all unique pairs of words\n",
    "for word1, word2 in combinations(words, 2):\n",
    "    # Extract vectors\n",
    "    word1_neural_vector = neural_sim_df_reordered[word1]\n",
    "    word2_neural_vector = neural_sim_df_reordered[word2]\n",
    "    word1_semantic_vector = word2vec_sim_df[word1]\n",
    "    word2_semantic_vector = word2vec_sim_df[word2]\n",
    "\n",
    "    # Remove indices with perfect correlations\n",
    "    perfect_corr_indices = (word1_neural_vector == 1.0) | (word2_neural_vector == 1.0) | \\\n",
    "                           (word1_semantic_vector == 1.0) | (word2_semantic_vector == 1.0)\n",
    "    word1_neural_vector_filtered = word1_neural_vector[~perfect_corr_indices]\n",
    "    word2_neural_vector_filtered = word2_neural_vector[~perfect_corr_indices]\n",
    "    word1_semantic_vector_filtered = word1_semantic_vector[~perfect_corr_indices]\n",
    "    word2_semantic_vector_filtered = word2_semantic_vector[~perfect_corr_indices]\n",
    "\n",
    "    # Calculate correlations\n",
    "    corr_word1_neural_semantic = pearsonr(word1_neural_vector_filtered, word1_semantic_vector_filtered)[0]\n",
    "    corr_word1_neural_word2_semantic = pearsonr(word1_neural_vector_filtered, word2_semantic_vector_filtered)[0]\n",
    "    corr_word2_neural_semantic = pearsonr(word2_neural_vector_filtered, word2_semantic_vector_filtered)[0]\n",
    "    corr_word2_neural_word1_semantic = pearsonr(word2_neural_vector_filtered, word1_semantic_vector_filtered)[0]\n",
    "\n",
    "    # Check decoding accuracy\n",
    "    decode_accuracy_word1 = corr_word1_neural_semantic > corr_word1_neural_word2_semantic\n",
    "    decode_accuracy_word2 = corr_word2_neural_semantic > corr_word2_neural_word1_semantic\n",
    "\n",
    "    # Append results\n",
    "    decoding_results.append({\n",
    "        'word1': word1,\n",
    "        'word2': word2,\n",
    "        'corr_word1_neural_semantic': corr_word1_neural_semantic,\n",
    "        'corr_word1_neural_word2_semantic': corr_word1_neural_word2_semantic,\n",
    "        'corr_word2_neural_semantic': corr_word2_neural_semantic,\n",
    "        'corr_word2_neural_word1_semantic': corr_word2_neural_word1_semantic,\n",
    "        'decode_accuracy_word1': decode_accuracy_word1,\n",
    "        'decode_accuracy_word2': decode_accuracy_word2\n",
    "    })\n",
    "\n",
    "# Convert results to a DataFrame\n",
    "decoding_results_df = pd.DataFrame(decoding_results)\n",
    "\n",
    "# Calculate the overall decoder accuracy from the results DataFrame\n",
    "total_accuracy = (decoding_results_df['decode_accuracy_word1'].sum() + \n",
    "                  decoding_results_df['decode_accuracy_word2'].sum())\n",
    "p9_overall_accuracy = total_accuracy / (2 * len(decoding_results_df))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "2722ab25",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.430225988700565\n",
      "0.4344632768361582\n",
      "0.481638418079096\n",
      "0.4124293785310734\n",
      "0.4401129943502825\n",
      "0.42909604519774014\n",
      "0.4652542372881356\n",
      "0.44124293785310736\n",
      "0.38305084745762713\n"
     ]
    }
   ],
   "source": [
    "print(f\"{p1_overall_accuracy}\")\n",
    "print(f\"{p2_overall_accuracy}\")\n",
    "print(f\"{p3_overall_accuracy}\")\n",
    "print(f\"{p4_overall_accuracy}\")\n",
    "print(f\"{p5_overall_accuracy}\")\n",
    "print(f\"{p6_overall_accuracy}\")\n",
    "print(f\"{p7_overall_accuracy}\")\n",
    "print(f\"{p8_overall_accuracy}\")\n",
    "print(f\"{p9_overall_accuracy}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b711608e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "anderson_baroni...DONE!\n"
     ]
    }
   ],
   "source": [
    "print('anderson_baroni...DONE!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8db91a4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
