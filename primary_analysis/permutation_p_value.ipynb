{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "17c33ade",
   "metadata": {},
   "source": [
    "The first two cells provide an example of how we shuffle the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e9bb98bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Load the semantic-model correlation matrix from GloVe_sim.csv\n",
    "semantic_model_df = pd.read_csv('GloVe_sim.csv')\n",
    "\n",
    "# Get the number of words\n",
    "num_words = len(semantic_model_df.columns) - 1  # Assuming the first column is not a word but an identifier like 'term'\n",
    "\n",
    "# Create a vector of word indices and shuffle it\n",
    "word_indices = np.arange(num_words)\n",
    "np.random.shuffle(word_indices)\n",
    "\n",
    "# Reorder the rows and columns of the semantic-model correlation matrix\n",
    "shuffled_semantic_model_df = semantic_model_df.iloc[:, 1:].iloc[word_indices, word_indices]\n",
    "\n",
    "# Add the 'term' column back to maintain the structure\n",
    "shuffled_semantic_model_df.insert(0, 'term', semantic_model_df['term'].iloc[word_indices])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9004fef",
   "metadata": {},
   "source": [
    "Now we create a big loop to do estimate our permutation p-value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6c4bcf8b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total execution time: 381.8034279346466 seconds\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from itertools import combinations\n",
    "from scipy.stats import pearsonr\n",
    "import time\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "# Load data\n",
    "word2vec_sim_df = pd.read_csv('GloVe_sim.csv')\n",
    "neural_sim_df = pd.read_csv('p1_sim.csv')\n",
    "\n",
    "# Align the columns and reorder the neural similarity matrix (if necessary)\n",
    "neural_sim_df_aligned = neural_sim_df[word2vec_sim_df.columns]\n",
    "neural_sim_df_reordered = neural_sim_df_aligned.set_index('term').reindex(word2vec_sim_df['term']).reset_index()\n",
    "\n",
    "# List of words (excluding the 'term' column)\n",
    "words = word2vec_sim_df.columns[1:]\n",
    "\n",
    "# Initialize a list to store decoding accuracies\n",
    "decoding_accuracies = []\n",
    "\n",
    "# Number of iterations for the permutation test\n",
    "num_iterations = 100  # Adjust this number as needed\n",
    "\n",
    "for iteration in range(num_iterations):\n",
    "    # Initialize a list to store decoding results for this iteration\n",
    "    decoding_results = []\n",
    "\n",
    "    # Shuffle the semantic-model correlation matrix\n",
    "    word_indices = np.arange(len(word2vec_sim_df.columns) - 1)\n",
    "    np.random.shuffle(word_indices)\n",
    "    shuffled_semantic_model_df = word2vec_sim_df.iloc[:, 1:].iloc[word_indices, word_indices]\n",
    "    shuffled_semantic_model_df.insert(0, 'term', word2vec_sim_df['term'].iloc[word_indices])\n",
    "\n",
    "    # Iterate over all unique pairs of words\n",
    "    for word1, word2 in combinations(words, 2):\n",
    "        # Extract vectors\n",
    "        word1_neural_vector = neural_sim_df_reordered[word1]\n",
    "        word2_neural_vector = neural_sim_df_reordered[word2]\n",
    "        word1_semantic_vector = shuffled_semantic_model_df[word1]\n",
    "        word2_semantic_vector = shuffled_semantic_model_df[word2]\n",
    "\n",
    "        # Remove indices with perfect correlations\n",
    "        perfect_corr_indices = (word1_neural_vector == 1.0) | (word2_neural_vector == 1.0) | \\\n",
    "                               (word1_semantic_vector == 1.0) | (word2_semantic_vector == 1.0)\n",
    "        word1_neural_vector_filtered = word1_neural_vector[~perfect_corr_indices]\n",
    "        word2_neural_vector_filtered = word2_neural_vector[~perfect_corr_indices]\n",
    "        word1_semantic_vector_filtered = word1_semantic_vector[~perfect_corr_indices]\n",
    "        word2_semantic_vector_filtered = word2_semantic_vector[~perfect_corr_indices]\n",
    "\n",
    "        # Calculate correlations\n",
    "        corr_word1_neural_semantic = pearsonr(word1_neural_vector_filtered, word1_semantic_vector_filtered)[0]\n",
    "        corr_word1_neural_word2_semantic = pearsonr(word1_neural_vector_filtered, word2_semantic_vector_filtered)[0]\n",
    "        corr_word2_neural_semantic = pearsonr(word2_neural_vector_filtered, word2_semantic_vector_filtered)[0]\n",
    "        corr_word2_neural_word1_semantic = pearsonr(word2_neural_vector_filtered, word1_semantic_vector_filtered)[0]\n",
    "\n",
    "        # Check decoding accuracy\n",
    "        decode_accuracy_word1 = corr_word1_neural_semantic > corr_word1_neural_word2_semantic\n",
    "        decode_accuracy_word2 = corr_word2_neural_semantic > corr_word2_neural_word1_semantic\n",
    "\n",
    "        # Append results\n",
    "        decoding_results.append({\n",
    "            'word1': word1,\n",
    "            'word2': word2,\n",
    "            'corr_word1_neural_semantic': corr_word1_neural_semantic,\n",
    "            'corr_word1_neural_word2_semantic': corr_word1_neural_word2_semantic,\n",
    "            'corr_word2_neural_semantic': corr_word2_neural_semantic,\n",
    "            'corr_word2_neural_word1_semantic': corr_word2_neural_word1_semantic,\n",
    "            'decode_accuracy_word1': decode_accuracy_word1,\n",
    "            'decode_accuracy_word2': decode_accuracy_word2\n",
    "        })\n",
    "\n",
    "    # Convert results to a DataFrame for this iteration\n",
    "    decoding_results_df = pd.DataFrame(decoding_results)\n",
    "\n",
    "    # Calculate decoding accuracy for this iteration\n",
    "    total_accuracy = (decoding_results_df['decode_accuracy_word1'].sum() + \n",
    "                      decoding_results_df['decode_accuracy_word2'].sum())\n",
    "    decoding_accuracy = total_accuracy / (2 * len(decoding_results_df))\n",
    "\n",
    "    # Append the result to the list\n",
    "    decoding_accuracies.append(decoding_accuracy)\n",
    "\n",
    "# Convert the list to a DataFrame\n",
    "decoding_accuracies_df = pd.DataFrame(decoding_accuracies, columns=['Decoding Accuracy'])\n",
    "\n",
    "# save csv\n",
    "decoding_accuracies_df.to_csv('perm_p1_GloVE_100.csv')\n",
    "end_time = time.time()\n",
    "\n",
    "duration = end_time - start_time\n",
    "print(f\"Total execution time: {duration} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "017338d8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
